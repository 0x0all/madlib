/* ----------------------------------------------------------------------- *//** 
 *
 * @file linear.sql_in
 *
 * @brief SQL functions for linear regression
 * @date January 2011
 *
 * @sa For a brief introduction to linear regression, see the module
 *     description \ref grp_linreg.
 *
 *//* ----------------------------------------------------------------------- */

m4_include(`SQLCommon.m4')

/**
@addtogroup grp_linreg

@about

Linear regression refers to a stochastic model, in which the conditional mean
of the dependent variable (usually denoted \f$ y \f$) is an affine function of
the vector of independent variables (usually denoted \f$ \boldsymbol x \f$):
\f[
    E[y \mid \boldsymbol x] = \boldsymbol c^T \boldsymbol x
\f]
for some unknown vector of coefficients \f$ \boldsymbol c \f$.

We fit the model with the ordinary-least-squares method. That is, the vector of
regression coefficients \f$ \boldsymbol c \f$ is estimated as:
\f[
    \boldsymbol c = (X^T X)^+ X^T \boldsymbol y = X^+ \boldsymbol y
\f]
where
- \f$ X \f$ is the design matrix with \f$ k \f$ columns and \f$ n \f$ rows,
  containing all observed vectors of independent variables
  \f$ \boldsymbol x_i \f$ as rows
- \f$ X^T \f$ denotes the transpose of \f$ X \f$
- \f$ X^+ \f$ denotes the pseudo-inverse of \f$ X \f$.
Note: The identity \f$ X^+ = (X^T X)^+ X^T \f$ holds for all matrices \f$ X \f$.
A proof can be found, e.g., at:
http://en.wikipedia.org/wiki/Proofs_involving_the_Moore%2DPenrose_pseudoinverse

Computing the <b>total sum of squares</b> \f$ TSS \f$, the <b>explained
sum of squares</b> \f$ ESS \f$ (also called the regression sum of
squares), and the <b>residual sum of squares</b> \f$ RSS \f$ (also called sum of
squared residuals or sum of squared errors of prediction) is
done according to the following formulas:
\f{align*}{
    ESS & = \boldsymbol y^T X \boldsymbol c
        -   \frac{ \| y \|_1^2 }{n} \\
    TSS & = \sum_{i=1}^k y_i^2
        -   \frac{ \| y \|_1^2 }{n} \\
    R^2 & = \frac{ESS}{TSS}
\f}
Note: The last equality follows from the definition
\f$ R^2 = 1 - \frac{RSS}{TSS} \f$ and the fact that for linear regression
\f$ TSS = RSS + ESS \f$. A proof of the latter can be found, e.g., at:
http://en.wikipedia.org/wiki/Sum_of_squares

We estimate the variance
\f$ Var[y - \boldsymbol c^T \boldsymbol x \mid \boldsymbol x] \f$ as
\f[
    \sigma^2 = \frac{RSS}{n - k}
\f]
and compute the t-statistic for coefficient \f$ i \f$ as
\f[
    t_i = \frac{c_i}{\sqrt{\sigma^2 \cdot \left( (X^T X)^{-1} \right)_{ii} }}
\f]

The \f$ p \f$-value for coefficient \f$ i \f$ gives the probability that the
null hypothesis (\f$ c_i = 0 \f$) is false, i.e., the probability that
\f$ c_i \f$ differs significantly from 0. Letting \f$ F_\nu \f$ denote the
cumulative density function of student-t with \f$ \nu \f$ degrees of freedom,
the \f$ p \f$-value for coefficient \f$ i \f$
is therefore
\f[
    p_i = P(|T| \geq |t_i|) = 2 \cdot (1 - F_{n - k}( |t_i| ))
\f]
where \f$ T \f$ is a student-t distributed random variable with mean 0.

@prereq

Implemented in C for PostgreSQL/Greenplum.

@usage

-# The data set is expected to be of the following form:\n
   <tt>{TABLE|VIEW} <em>sourceName</em> ([...] <em>dependentVariable</em>
   DOUBLE PRECISION, <em>independentVariables</em> DOUBLE PRECISION[],
   [...])</tt>  
-# Run the linear regression by:\n
   <tt>SELECT (\ref linregr(float8,float8[]) "linregr"(<em>dependentVariable</em>,
   <em>independentVariables</em>)).* FROM <em>sourceName</em></tt>\n
   Note: In order to model an intercept, set one coordinate in the
   <tt>independentVariables</tt> array to 1. (See below for an example.)

@examp

The following examples is taken from
http://www.stat.columbia.edu/~martin/W2110/SAS_7.pdf.

@verbatim
# select * from houses;
 id | tax  | bedroom | bath | price  | size |  lot  
----+------+---------+------+--------+------+-------
  1 |  590 |       2 |    1 |  50000 |  770 | 22100
  2 | 1050 |       3 |    2 |  85000 | 1410 | 12000
  3 |   20 |       3 |    1 |  22500 | 1060 |  3500
  4 |  870 |       2 |    2 |  90000 | 1300 | 17500
  5 | 1320 |       3 |    2 | 133000 | 1500 | 30000
  6 | 1350 |       2 |    1 |  90500 |  820 | 25700
  7 | 2790 |       3 |  2.5 | 260000 | 2130 | 25000
  8 |  680 |       2 |    1 | 142500 | 1170 | 22000
  9 | 1840 |       3 |    2 | 160000 | 1500 | 19000
 10 | 3680 |       4 |    2 | 240000 | 2790 | 20000
 11 | 1660 |       3 |    1 |  87000 | 1030 | 17500
 12 | 1620 |       3 |    2 | 118600 | 1250 | 20000
 13 | 3100 |       3 |    2 | 140000 | 1760 | 38000
 14 | 2070 |       2 |    3 | 148000 | 1550 | 14000
 15 |  650 |       3 |  1.5 |  65000 | 1450 | 12000
(15 rows)

# select (madlib.linregr(price, array[1, bedroom, bath, size])).coef from houses;
                                  coef                                  
------------------------------------------------------------------------
 {27923.4332082674,-35524.7752262529,2269.34397934633,130.793920087895}
(1 row)

# select (madlib.linregr(price, array[1, bedroom, bath, size])).r2 from houses;
        r2         
-------------------
 0.745374009992022
(1 row)
@endverbatim

The complete output of \ref linregr(float8,float8[]) "linregr" is available by:
@verbatim
# select (madlib.linregr(price, array[1, bedroom, bath, size])).* from houses;
@endverbatim

@sa file linear.sql_in (documenting the SQL functions)

@internal
@sa file linear.cpp (documenting the implementation in C++), function
    \link LinearRegression (documenting the formulas used for coefficients,
    $R^2$, t-statistics, and p-values, implemented in C++)
@endinternal

@literature

[1] Cosma Shalizi: Statistics 36-350: Data Mining, Lecture Notes, 21 October
    2009, http://www.stat.cmu.edu/~cshalizi/350/lectures/17/lecture-17.pdf

*/

CREATE TYPE MADLIB_SCHEMA.linregr_result AS (
    coef DOUBLE PRECISION[],
    r2 DOUBLE PRECISION,
    std_err DOUBLE PRECISION[],
    t_stats DOUBLE PRECISION[],
    p_values DOUBLE PRECISION[]
);

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.linregr_transition(
    state DOUBLE PRECISION[],
    y DOUBLE PRECISION,
    x DOUBLE PRECISION[])
RETURNS DOUBLE PRECISION[]
AS 'MODULE_PATHNAME'
LANGUAGE C
IMMUTABLE STRICT;


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.linregr_merge_states(
    state1 DOUBLE PRECISION[],
    state2 DOUBLE PRECISION[])
RETURNS DOUBLE PRECISION[]
AS 'MODULE_PATHNAME'
LANGUAGE C
IMMUTABLE STRICT;


-- Final functions

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.linregr_final(
    state DOUBLE PRECISION[])
RETURNS MADLIB_SCHEMA.linregr_result
AS 'MODULE_PATHNAME'
LANGUAGE C IMMUTABLE STRICT;


/**
 * @brief Compute linear regression coefficients and diagnostic statistics.
 *
 * To include an intercept in the model, set one coordinate in the
 * <tt>independentVariables</tt> array to 1.
 *
 * @return A tuple containing:
 *     - \c coef Array of coefficients, which has the same length as the array of
 *     independent variables.
 *     - \c r2 Coefficient of determination, \f$ R^2 \f$
 *     - \c std_err Array of standard errors for each coefficient
 *     - \c t_stats Array of t-statistics for each coefficient
 *     - \c p_values Array of p-values, for every coefficient
 *
 * @examp <tt>SELECT (linregr(y, array[1, x1, x2])).* FROM data;</tt>
 */
CREATE AGGREGATE MADLIB_SCHEMA.linregr(
    /*+ "dependentVariable" */ DOUBLE PRECISION,
    /*+ "independentVariables" */ DOUBLE PRECISION[]) (
    
    SFUNC=MADLIB_SCHEMA.linregr_transition,
    STYPE=float8[],
    FINALFUNC=MADLIB_SCHEMA.linregr_final,
    m4_ifdef(`GREENPLUM',`prefunc=MADLIB_SCHEMA.linregr_merge_states,')
    INITCOND='{0,0,0,0,0}'
);
