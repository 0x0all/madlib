# coding=utf-8

"""
@file kmeans.py_in

@brief k-Means Clustering module

@namespace kmeans
"""

import time
import plpy
from math import floor, log, pow
import os, sys

# ----------------------------------------
# K-means global variables
# ----------------------------------------
global verbose, output_points, output_centroids

# ----------------------------------------
# Logging
# ----------------------------------------
def info( msg):
    if verbose: 
        plpy.info( msg)
   
# ----------------------------------------
# Quotes a string to be used as an identifier 
# ----------------------------------------
#def quote_ident(val):
#    return '"' + val.replace('"', '""') + '"'

# ----------------------------------------
# Quotes a string to be used as a literal 
# ----------------------------------------
#def quote_literal(val):
#    return "'" + val.replace("'", "''") + "'"

# ----------------------------------------
# Time a SQL statement
# ----------------------------------------
def __timed_execute(sql):
    start = time.time()
    result = plpy.execute( sql);	  
    time_sec = round( time.time() - start, 3)         
    return result, time_sec;

# ----------------------------------------
# Run SQL in ERROR only message mode
# ----------------------------------------
def __run_quietly(sql):

    # Remember current message level and set it to ERROR
    prev_msg_level = plpy.execute("SELECT setting FROM pg_settings " \
        + " WHERE name='client_min_messages'")[0]['setting']
    plpy.execute("SET client_min_messages = error;")

    # Run...
    rv = plpy.execute(sql)

    # Restrore previous message level
    plpy.execute("SET client_min_messages = %s;" % prev_msg_level)
    
    return rv
    
# ----------------------------------------
# Centroid initialization using random()
# ----------------------------------------
def __init_random( madlib_schema, src_view, k, p):
    """
    Creates the initial set of centroids using random().

    """
	
    # Find k random points
    # Using Chernoff bound derived solution:
    # p >= ( s + 14 + sqrt(196 + 28s) ) / n
    # where:
    #  p - probablity
    #  s - desired number of samples
    #  n - population size
    sql = 'INSERT INTO ' + output_centroids + ' (cid, coords)' \
            + ' SELECT row_number() OVER () AS cid, coords FROM' \
            + ' ( SELECT coords' \
            + '   FROM ' + src_view \
            + '   WHERE random() < (( ' + str(k) + ' + 14 + sqrt( 196 + 28 * ' \
            + str(k) + ') ) / ' + str(p) + ')' \
            + '   ORDER BY random() LIMIT ' + str(k) \
            + ' ) q';
    plpy.execute( sql);

    # Return # of created centroids
    rv = plpy.execute( 'SELECT count(*) AS cnt FROM ' + output_centroids);
    return rv[0]['cnt']
    
# ----------------------------------------
# Centroid initialization using kmeans++
# ----------------------------------------
def __init_plusplus( madlib_schema, src_view, k, p, dist_func):
    """
    Creates the initial set of centroids using kmeans++.

    """
	
    # Find the 1st centroid
    # info( '...seeding ' + str(k) + ' centroids using kmeans++...');
    sql = '''    
        INSERT INTO ''' + output_centroids + ''' (cid, coords) 
        SELECT 1, coords::''' + madlib_schema + '''.SVEC 
        FROM ''' + src_view + '''
        WHERE random() < 0.1 LIMIT 1''';
    plpy.execute( sql);

    # Find remaining centroids
    i = 1
    while (i < k):
        i = i + 1;
        sql = '''
            INSERT INTO ''' + output_centroids + ''' (cid, coords) 
            SELECT ''' + str(i) + ''', pt.coords 
            FROM 
                ''' + src_view + ''' pt, 
                ( 
                -- Find a random point 
                -- that is furthest away from current Centroids
                SELECT p.pid, min( 
                        ''' + dist_func.replace( '&&&', 'p.coords, c.coords') + '''^2
                        * random()
                    ) AS distance 
                FROM 
                    ( SELECT coords, pid FROM ''' + src_view + '''
                      WHERE random() < (( ''' + str(k) + ' + 14 + sqrt( 196 + 28 * ' + str(k) + ') ) / ' + str(p) + ''')
                      ORDER BY random() LIMIT ''' + str(k) + '''
                    ) AS p -- K random points 
                    CROSS JOIN 
                    (SELECT coords FROM ''' + output_centroids + ''') 
                    AS c -- current centroids 
                GROUP BY p.pid ORDER BY distance DESC LIMIT 1 
                ) AS pc 
            WHERE pc.pid = pt.pid''';
        plpy.execute( sql);

    # Return # of created centroids
    rv = plpy.execute( 'SELECT count(*) AS cnt FROM ' + output_centroids);
    return rv[0]['cnt']

# ----------------------------------------
# Centroid initialization using canopy
# ----------------------------------------
def __init_canopy( madlib_schema, src_view, p, dist_metric, dist_func, t1, t2):
    """
    Creates the initial set of centroids using canopy clustering.

    """
	
    # Find thresholds T1 and T2 (T1 < T2)
    if t1 is None or t2 is None:
        # Random sample size 
        sample = 100;
        # Prepare the table
        sql = 'CREATE TEMP TABLE random_points( ' \
            + 'pid BIGINT, coords ' + madlib_schema + '.svec)';
        __run_quietly( sql);
        # Load some random points
        sql = 'INSERT INTO random_points' \
            + ' SELECT pid, coords FROM ' + src_view \
            + ' WHERE random() < (( ' + str(sample) + ' + 14 + sqrt( 196 + 28 * ' + str(sample) + ') ) / ' + str(p) + ')' \
            + ' ORDER BY random() LIMIT ' + str(sample);
        plpy.execute( sql);
        # Calculate the cross distances >> and dervie T1 and T2 
        # T1 >> is the max value from 1st quintile (10 buckets)
        # T2 >> is the min value from 10th quintile (10 buckets)
        sql = 'SELECT ' \
            + ' max( CASE WHEN ntile = 1 THEN dist ELSE null END) as t1 ' \
            + ' , min( CASE WHEN ntile = 10 THEN dist ELSE null END) as t2 ' \
            + 'FROM ' \
            + '(' \
            + ' SELECT ntile(10) OVER( ORDER BY dist) as ntile, dist FROM ' \
            + '  (' \
            + '   SELECT ' + dist_func.replace( '&&&', 'a.coords, b.coords') + ' AS dist' \
            + '   FROM random_points a, random_points b' \
            + '   WHERE a.pid < b.pid' \
            + '  ) q1' \
            + ') q2 ' \
            + 'WHERE ntile in (1,10) ';
        rv = plpy.execute( sql);
        
    if t1 is None: t1 = rv[0]['t1'];
    if t2 is None: t2 = rv[0]['t2'];
    
    # Generate initial centroids using T1 (smaller)
    sql = 'INSERT INTO ' + output_centroids + ' (cid, coords)' \
        + ' SELECT row_number() OVER () AS cid, coords ' \
        + ' FROM' \
            + '     ( SELECT unnest(canopies) as coords' \
        + '       FROM ' \
        + '           ( SELECT ' + madlib_schema + ".kmeans_canopy( coords, '" \
        + dist_metric + "', " + str(float(t1)) + ') as canopies' \
        + '             FROM ' + src_view \
        + '           ) q1' \
        + '     ) q2';
    plpy.execute( sql);

    # Assign points to canopies using T2 (larger)
    __run_quietly( 'CREATE TEMP TABLE TempPoints0_Canopies (LIKE TempPoints0)')
    sql = 'INSERT INTO TempPoints0_Canopies' \
        + ' SELECT p.pid, p.coords, p.cid, array_agg(c.cid)' \
        + ' FROM TempPoints0 p, ' + output_centroids + ' c ' \
        + ' WHERE ' + dist_func.replace( '&&&', 'p.coords, c.coords') + ' < ' + str(t2) \
        + ' GROUP BY 1,2,3';            
    plpy.execute( sql);
    plpy.execute( 'DROP TABLE TempPoints0');
    plpy.execute( 'ALTER TABLE TempPoints0_Canopies RENAME TO TempPoints0');
    
    # Return # of created centroids
    rv = plpy.execute( 'SELECT count(*) AS cnt FROM ' + output_centroids);
    return rv[0]['cnt'], t1, t2;
    
# ------------------------------------------------------------------------------
# Main function to run the k-means algorithm
# ------------------------------------------------------------------------------
def kmeans( madlib_schema
            , src_relation, src_col_data, src_col_id
            , init_cset_rel, init_cset_col
            , init_method, k, t1, t2, dist_metric
            , max_iter, conv_threshold, evaluate 
            , out_points, out_centroids, overwrite
            , p_verbose):
            
    """
    Executes the k-means clustering algorithm.
    
    """

    # Global variables
    global verbose, output_points, output_centroids
    
    # Some defaults
    if p_verbose is None:
        verbose = False;
    else:
        verbose = p_verbose;
    output_points = out_points
    output_centroids = out_centroids

    # Maximum number of iterations
    if max_iter > 0:
        max_iterations = max_iter;        
    else:
        max_iterations = 20;    # default

    # Convergence threshold (% of points that changed assignments)
    convergence_log = [100.0];
    if conv_threshold > 0:
        convergence_threshold = conv_threshold;
    else:
        convergence_threshold = 0.001;  # default
        
    #
    # Non-Adjustable Variables 	
    #
    point_count = 0;            # number of input points
    centr_count = 0;            # initial number of centroids
    centr_final_count = 0;      # final number of centroids
    done = False;               # loop control variable
    gfit = 0;                   # goodness of fit measure

    info( 'Started K-means clustering with parameters:')

    #   
    # Validate all parameters
    #
    
    # Validate: src_relation
    try:
        plpy.execute( "SELECT * FROM " + src_relation + " LIMIT 1")
    except:
        plpy.error( 'source relation "%s" does not exist' % src_relation )
    info( ' * src_relation = %s' % src_relation)

    # Validate: src_col_data
    try:
        plpy.execute( "SELECT %s FROM %s LIMIT 1" % (src_col_data, src_relation))
    except:
        plpy.error( 'column "%s" not found in relation "%s"'
                    % (src_col_data, src_relation) )
    info( ' * src_col_data = %s' % src_col_data)

    # Validate: src_col_id
    if src_col_id is not None:
        try:
            plpy.execute( "SELECT %s FROM %s LIMIT 1" % (src_col_id, src_relation))
            has_pid = 1;
            info( ' * src_col_id = %s' % (src_col_id) )
        except:
            plpy.error( 'column "%s" not found in relation "%s"' 
                        % (src_col_id, src_relation) )
    else:
        has_pid = 0;
        info( ' * src_col_id = None (will be auto-generated)')
    
        
    # Validate: init_cset_rel and init_cset_col
    if (init_cset_rel is not None) and (init_cset_col is not None):
        try:
            plpy.execute( "SELECT * FROM %s LIMIT 1" % init_cset_rel)
        except:
            plpy.error( 'relation "%s" does not exist' % init_cset_rel )
        
        info( ' * init_cset_rel = %s' % init_cset_rel)
        try:
            rv = plpy.execute( "SELECT count(" + init_cset_col + ") FROM " + init_cset_rel)
        except:
            plpy.error( 'column "%s" not found in relation "%s"' 
                        % (init_cset_col, init_cset_rel) )
        if rv[0]['count'] > 0:
            cset_count = rv[0]['count']        
            k = None;
        else:
            plpy.error( "initial centroid relation (%s) is empty" % init_cset_rel)
        info( ' * init_cset_col = %s' % init_cset_col)  
        
    # Validate: init_method & no-k (canopy)
    elif init_method == 'canopy':
        info( ' * init_method = %s (t1=%s, t2=%s)' % (init_method, t1, t2));
        k = None;
        cset_count = None;

    # Validate: init_method & k (other methods)
    elif k > 0:
        if init_method is None:
            init_method = 'random';
            info( ' * init_method = None (default: %s)' % init_method)
        elif init_method.lower() == 'random':
            init_method = 'random';
            info( ' * init_method = %s' % init_method)
        elif init_method.lower() == 'kmeans++':
            init_method = 'kmeans++';
            info( ' * init_method = %s' % init_method)
        else:
            init_method = 'random';
            info( ' * init_method = Unknown (default: %s)' % init_method)
        info( ' * initial k = %s' % k)
        cset_count = None;
    # Otherwise
    elif k <= 0:
        plpy.error( "k (" + str(k) + ") must be a positive integer")
    else:        
        plpy.error( "either k or initial centroid set must be specified")

    # Validate: dist_metric
    dist_metric = dist_metric.lower();
    # Euclidean/L2norm
    if dist_metric == 'euclidean' or dist_metric == 'l2norm':
        dist_aggr = '%s.avg(&&&)' % madlib_schema;
        dist_func = '%s.l2norm(&&&)' % madlib_schema;
        info( ' * dist_metric = %s' % (dist_metric))
    # Manhattan/L1norm
    elif dist_metric == 'manhattan' or dist_metric == 'l1norm':
        dist_aggr = '%s.avg(&&&)' % madlib_schema;
        dist_func = '%s.l1norm(&&&)' % madlib_schema;
        info( ' * dist_metric = %s' % (dist_metric))
    # Cosine
    elif dist_metric == 'cosine':
        dist_aggr = '%s.avg(%s.normalize(&&&))' % (madlib_schema, madlib_schema);
        dist_func = 'acos(%s.cosine(&&&))' % madlib_schema;
        info( ' * dist_metric = %s' % (dist_metric))
    # Tanimoto
    elif dist_metric == 'tanimoto':
        dist_aggr = '%s.avg(%s.normalize(&&&))' % (madlib_schema, madlib_schema);
        dist_func = 'acos(%s.tanimoto(&&&))' % madlib_schema;
        info( ' * dist_metric = %s' % (dist_metric))
    # Other
    else: 
        dist_metric = 'l2norm';
        dist_aggr = '%s.avg(&&&)' % madlib_schema;
        dist_func = '%s.l2norm(&&&)' % madlib_schema;
        info( ' * dist_metric = Unknown (default: %s)' % (dist_metric))

    # Validate: k/cset VS data_points
    rv = plpy.execute( "SELECT count(*) as cnt FROM " + src_relation);
    point_count = rv[0]['cnt'];
    if (point_count == 0):
        plpy.error( "source relation is empty (" + src_relation + ")");
    elif (cset_count > 0 and point_count < cset_count):
        plpy.error( "there is more initial centroids (%s) than data points (%s)" 
                    % (cset_count, point_count));
    elif (k > 0 and point_count < k):
        plpy.error( "more initial centroids specified () than data points" 
                    % (k, point_count));
        
    # Validate: evaluate
    if evaluate is None: evaluate == True; 
    if (evaluate == True):
        info( ' * evaluate = %s (model coefficient calculation)' % str(evaluate));
    elif (evaluate == False):
        info( ' * evaluate = %s (model coefficient evaluation)' % str(evaluate));
        
    # Validate: output_points
    if overwrite is None: overwrite = False;
    if overwrite == True:
        __run_quietly( 'DROP TABLE IF EXISTS ' + output_points + ' CASCADE')
    try:
        sql = "CREATE TABLE " + output_points + " (" + \
              " pid BIGINT," + \
              " coords " + madlib_schema + ".SVEC," + \
              " cid INT )";
        __run_quietly( sql)
    except:
        plpy.error( "output table %s already exists\n" % output_points )
    info( ' * output_points = %s' % output_points)

    # Validate: output_centroids
    if overwrite == True:
        __run_quietly( 'DROP TABLE IF EXISTS ' + output_centroids + ' CASCADE')
    try:
        sql = "CREATE TABLE " + output_centroids + " (" \
            + " cid INT," \
            + " coords " + madlib_schema + ".SVEC )" 
        __run_quietly( sql)
    except:
        if overwrite == True:
            __run_quietly( 'DROP TABLE ' + output_centroids + ' CASCADE')
            __run_quietly( sql)        
        else:
            plpy.error( "output table %s already exists\n" % output_centroids )
    info( ' * output_centroids = %s' % output_centroids)

    # Validate: verbose 
    if verbose:
        info( ' * verbose = True')

    # Create a view with PID and COORDS columns
    src_view = 'tmp_madlib_view_' + str(os.getpid());
    src_type = madlib_schema + '.svec';
    # Depending on existence of source ID column
    if (has_pid == 0):
        # Generate a new one        
        sql = "CREATE VIEW " + src_view \
            + " AS SELECT row_number() over() AS pid, " \
            + src_col_data +  "::" + src_type + " AS coords" \
            + " FROM " + src_relation 
        __run_quietly( sql)
    else:
        # Use the current one
        sql = "CREATE VIEW " + src_view \
            + " AS SELECT " + src_col_id + " as pid, " \
            + src_col_data +  "::" + src_type + " AS coords" \
            + " FROM " + src_relation
        __run_quietly( sql)
                	
	# Prepare the data set (skip all data points with any NULL values)
    __run_quietly( 'DROP TABLE IF EXISTS TempPoints0');	    
    sql = '''
        CREATE TEMP TABLE TempPoints0(
            pid BIGINT, 
            coords ''' + madlib_schema + '''.SVEC, 
            cid INTEGER,
            canopies INTEGER[]
        )
    ''';
    __run_quietly( sql);
    sql = '''
        INSERT INTO TempPoints0 
        SELECT pid, coords, 0, null
        FROM ''' + src_view + '''
        WHERE ''' + madlib_schema + '.svec_dot(coords,coords) IS NOT NULL';
    plpy.execute( sql);	    
    # Find the number of points after skipping NULL values
    sql = 'SELECT count(*) as cnt FROM TempPoints0';
    rv = plpy.execute( sql);    
    orig_point_count = point_count;	  
    point_count = rv[0]['cnt'];  
    # Find the number of dimensions
    sql = 'SELECT ' + madlib_schema + '.svec_dimension(coords) as dim FROM ' \
        + src_view + ' LIMIT 1';
    rv = plpy.execute( sql);	    
    info( 'Input:');
    info( ' * points: ' + str(orig_point_count) + ' (' \
            + str(rv[0]['dim']) + ' dimensions) >> kept ' + str(point_count) + \
            ' after removing NULLs');

    # Initialize centroids
    if cset_count > 0:
        info( ' * centroids: %s provided in %s' % (cset_count, init_cset_rel));
        sql = 'INSERT INTO ' + output_centroids + ' (cid, coords) ' \
            + ' SELECT row_number() OVER () AS cid, ' \
            + init_cset_col + '::' + madlib_schema + '.SVEC ' \
            + ' AS coords FROM ' + init_cset_rel
        plpy.execute( sql); 
        centr_count = cset_count;
        
    elif k > 0 and init_method == 'kmeans++':        
        start = time.time()
        centr_count = __init_plusplus( madlib_schema, src_view, k, point_count, dist_func);
        time_sec = round( time.time() - start, 3)
        info( ' * centroids: %s seeded using kmeans++ (%s sec)' 
               % (centr_count, str(time_sec)));
        
    elif k > 0 and init_method == 'random':
        start = time.time();
        centr_count = __init_random( madlib_schema, src_view, k, point_count);
        time_sec = round( time.time() - start, 3)
        info( ' * centroids: %s seeded using random selection (%s sec)' 
               % (centr_count, str(time_sec)));

    # decide whether to disregard k or not when "canopy" chosen
    elif init_method == 'canopy':
        start = time.time();
        centr_count, t1, t2 = __init_canopy( madlib_schema, src_view, point_count, dist_metric, dist_func, t1, t2);
        time_sec = round( time.time() - start, 3)
        info( ' * centroids: %s seeded using canopy clustering, t1=%s, t2=%s (%s sec)' 
               % (centr_count, t1, t2, str(time_sec)));

    else:
        error( 'Something went wrong, check: cset_count=%s, k=%s, init_method=%s' 
                % (cset_count, k, init_method));        

	
    # Main Loop
    info( 'Execution:')
    i = 0;
    while (done == False):	

        start = time.time();

        # Loop index
        i = i + 1;                      		

        # Create a temporary array of centroids
        __run_quietly( 'DROP TABLE IF EXISTS TempArrayOfCentroids');	
        sql = '''
            CREATE TEMP TABLE TempArrayOfCentroids AS
            -- CREATE TABLE TempArrayOfCentroids AS
            SELECT 
                array( 
                    SELECT cid 
                    FROM
                        ''' + output_centroids + ''' c
                    ORDER BY cid
                    LIMIT ''' + str(centr_count) + '''
                ) as cids,
                array( 
                    SELECT coords 
                    FROM
                        ''' + output_centroids + ''' c
                    ORDER BY cid
                    LIMIT ''' + str(centr_count) + '''
                ) as ccoords
        ''';
        __run_quietly( sql);	
    		
		# Create new temp table (i)
        __run_quietly( 'DROP TABLE IF EXISTS TempPoints' + str(i));
        __run_quietly( 'CREATE TEMP TABLE TempPoints%s (like TempPoints%s)' 
                        % (str(i), str(i-1)));

        # For each point assign the closest centroid
        if init_method == 'canopy':
            # Use canopies for proximity
            canopies = 'p.canopies';
        else:
            # Compare with all centroids
            canopies = 'arr.cids';
		    
        sql = '''
            INSERT INTO TempPoints%s	
            SELECT
                p.pid 
                , p.coords
                , %s.internal_kmeans_closest_centroid( p.coords, %s, arr.ccoords, '%s') 
                , p.canopies
            FROM 
                TempPoints%s p CROSS JOIN TempArrayOfCentroids arr
        ''' % (str(i), madlib_schema, canopies, dist_metric, str(i-1));
        plpy.execute( sql);	    

        # Refresh the Centroids table based on current assignments
        __run_quietly( 'DROP TABLE IF EXISTS ' + output_centroids + '_tmp');
        sql = '''
            CREATE TABLE ''' + output_centroids + '''_tmp AS
            SELECT c.cid AS cid, t.coords AS coords
            FROM ''' + output_centroids + ''' c LEFT OUTER JOIN
            (
                SELECT 
                    ''' + dist_aggr.replace( '&&&', 'coords') + ''' AS coords
                    , cid AS cid 
                FROM TempPoints''' + str(i) + ''' 
                GROUP BY cid
            ) AS t ON c.cid = t.cid''';
        #info (sql);
        __run_quietly( sql);	    
        plpy.execute( 'TRUNCATE TABLE ' + output_centroids );
        plpy.execute( 'INSERT INTO ' + output_centroids + ' SELECT * FROM ' \
                        + output_centroids + '_tmp');
        plpy.execute( 'DROP TABLE ' + output_centroids + '_tmp');
                        
        # Calculate the number of points that changed the assignment
        sql = '''
            SELECT count(*) as sum 
            FROM
                (
                SELECT t1.pid, t1.cid FROM TempPoints''' + str(i) + ''' t1
                EXCEPT
                SELECT t2.pid, t2.cid FROM TempPoints''' + str(i-1) + ''' t2
                ) t 
        ''';
        rv = plpy.execute( sql);
        time_sec = round( time.time() - start, 3)
        info( '... Iteration %s: updated %s points (%s sec)' \
                % (str(i), str(rv[0]['sum']), str(time_sec)));
        
		# Drop previous temp table (i-1)
        __run_quietly( 'DROP TABLE TempPoints' + str(i-1));
        
        # Add it to the tracking variable
        if (i>1): convergence_log.append( rv[0]['sum'] / (point_count * 1.0));

        # Exit conditions:
        if (i>5) and (convergence_log[i-6] - convergence_log[i-1] < 0): 
            done = True;
            info( 'Exit condition: fraction of reassigned nodes has stabilized around ' \
                    + str(( convergence_log[i-4] + convergence_log[i-3] + convergence_log[i-2] + convergence_log[i-1]) / 4.0));            
        elif (convergence_log[i-1] < convergence_threshold):
            done = True;
            info( 'Exit condition: fraction of reassigned nodes is smaller than: ' + str(convergence_threshold));
        elif (i == max_iterations):
            done = True;
            info( 'Exit condition: reached maximum number of iterations = ' + str(max_iterations));
            
    # Main Loop - END
    
    info( 'Writing final output table: ' + output_points + '...');
    sql = '''
        INSERT INTO ''' + output_points + '''	
        SELECT pid, coords, cid 
        FROM TempPoints''' + str(i);
    rv, time_sec = __timed_execute( sql);	  
    info( '... %s sec' % time_sec);

    # Evaluate the model
    # 1) Cost function value
    # 2) Simplified Silhouette coefficient:
    #    For details see: http://airccse.org/journal/ijdms/papers/3111ijdms03.pdf
    if evaluate == True :    
        info( 'Calculating model cost function and simplified Silhouette coefficient...');
        sql = ' SELECT' \
            + '   sum(a) AS cost,' \
            + '   coalesce( avg( (b-a) / float8larger(a,b) ), 0) AS scoef' \
            + ' FROM' \
            + '       ( SELECT ' \
            + '           pid, min(a)::float AS a, min(b)::float AS b' \
            + '         FROM' \
            + '           ( SELECT ' \
            + '               pid' \
            + '               , CASE WHEN p.cid = c.cid THEN ' \
            +                     dist_func.replace( '&&&', 'p.coords, c.coords') \
            + '                   ELSE null END AS a' \
            + '               , CASE WHEN p.cid != c.cid THEN ' \
            +                     dist_func.replace( '&&&', 'p.coords, c.coords') \
            + '                   ELSE null END AS b' \
            + '             FROM ' + output_points + ' p, ' + output_centroids + ' c' \
            + '           ) q1' \
            + '         GROUP BY pid' \
            + '       ) q2';
        rv, time_sec = __timed_execute( sql);
        info( '... %s sec' % time_sec);
        cost = rv[0]['cost'];
        scoef = rv[0]['scoef'];
    else:
        cost = None;
        scoef = None;        
        
    # Count final number of centroids        
    rv = plpy.execute( 'SELECT count(*) as count FROM ' + output_centroids);
    centr_final_count = rv[0]['count'];
        
    # Drop the input view
    rv = plpy.execute( "DROP VIEW " + src_view );  

    # Set some output values for provided centroid sets
    if init_method is None:
        k = cset_count;
        init_method = 'provided set';
    
    # Return output result
    return ( 
        src_relation,   
        point_count, 
        init_method,
        k,
        dist_metric,
        i,
        centr_final_count, 
        cost,
        scoef,
        output_points, 
        output_centroids
    )
