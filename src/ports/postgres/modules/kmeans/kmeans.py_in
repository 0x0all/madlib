# coding=utf-8

"""
@file kmeans.py_in

@brief k-Means Clustering module

@namespace kmeans
"""

import datetime
import plpy
from math import floor, log, pow
import os

# ----------------------------------------
# K-means global variables
# ----------------------------------------
global verbose, output_points, output_centroids

# ----------------------------------------
# Logging
# ----------------------------------------
def info( msg):
    if verbose: 
        plpy.info( msg)
   
# ----------------------------------------
# Quotes a string to be used as an identifier 
# ----------------------------------------
#def quote_ident(val):
#    return '"' + val.replace('"', '""') + '"'

# ----------------------------------------
# Quotes a string to be used as a literal 
# ----------------------------------------
#def quote_literal(val):
#    return "'" + val.replace("'", "''") + "'"

# ----------------------------------------
# Run SQL in ERROR only message mode
# ----------------------------------------
def __run_quietly(sql):

    # Remember current message level and set it to ERROR
    prev_msg_level = plpy.execute("SELECT setting FROM pg_settings " \
        + " WHERE name='client_min_messages'")[0]['setting']
    plpy.execute("SET client_min_messages = error;")

    # Run...
    rv = plpy.execute(sql)

    # Restrore previous message level
    plpy.execute("SET client_min_messages = %s;" % prev_msg_level)
    
    return rv

# ----------------------------------------
# Centroid initialization using random()
# ----------------------------------------
def __init_random( madlib_schema, src_view, k, p):
    """
    Creates the initial set of centroids using random().

    XXX - add parameters info
    """
	
    # Find k random points
    sql = 'INSERT INTO ' + output_centroids + ' (cid, coords) ' \
            + ' SELECT row_number() OVER () AS cid, coords' \
            + ' FROM ' + src_view \
            + ' WHERE random() < (' + str(k/p + 0.1) + ') LIMIT ' + str(k);
    plpy.execute( sql);

    # Return # of created centroids
    rv = plpy.execute( 'SELECT count(*) AS cnt FROM ' + output_centroids);
    return rv[0]['cnt']
    
# ----------------------------------------
# Centroid initialization using kmeans++
# ----------------------------------------
def __init_plusplus( madlib_schema, src_view, k, p):
    """
    Creates the initial set of centroids using kmeans++.

    XXX - add parameters info
    """
	
    # Find how many points we need to look through to be 99.9% sure that
    # we have seen a point from each cluster 
    numCentroids = floor( - log( 1 - pow( 0.999, 1/float(k))) * k);
    

    # Find the 1st centroid
    # info( '...seeding ' + str(k) + ' centroids using kmeans++...');
    sql = '''    
        INSERT INTO ''' + output_centroids + ''' (cid, coords) 
        SELECT 1, coords::''' + madlib_schema + '''.SVEC 
        FROM ''' + src_view + '''
        WHERE random() < 0.1 LIMIT 1''';
    plpy.execute( sql);

    # Find remaining centroids
    i = 1
    while (i < k):
        i = i + 1;
        sql = '''
            INSERT INTO ''' + output_centroids + ''' (cid, coords) 
            SELECT ''' + str(i) + ''', pt.coords 
            FROM 
                ''' + src_view + ''' pt, 
                ( 
                -- Find a random point 
                -- that is furthest away from current Centroids
                SELECT p.pid, min( ''' + madlib_schema + \
                '''.svec_l2norm(p.coords operator(''' + madlib_schema + \
                '''.-) c.coords)) * (random()^(0.2) ) AS distance 
                FROM 
                    (SELECT coords, pid FROM ''' + src_view + \
                    ''' WHERE random() < (''' + str(k/p + 0.1) + ''') 
                      LIMIT ''' + str(k) + ''') AS p -- K random points 
                    CROSS JOIN 
                    (SELECT coords FROM ''' + output_centroids + ''') 
                    AS c -- current centroids 
                GROUP BY p.pid ORDER BY distance DESC LIMIT 1 
                ) AS pc 
            WHERE pc.pid = pt.pid''';
        plpy.execute( sql);

    # Return # of created centroids
    rv = plpy.execute( 'SELECT count(*) AS cnt FROM ' + output_centroids);
    return rv[0]['cnt']
    
# ----------------------------------------
# Function to run the k-means algorithm
# ----------------------------------------
def kmeans( madlib_schema
            , src_relation, src_col_data, src_col_id
            , init_cset_rel, init_cset_col
            , k, init_method
            , max_iter, conv_threshold, gof #, sample_pct
            , out_points, out_centroids, overwrite
            , p_verbose):
            
    """
    Executes the k-means clustering algorithm.
    
    XXX - add parameters info
    """

    # Global variables
    global verbose, output_points, output_centroids
    verbose = p_verbose
    output_points = out_points
    output_centroids = out_centroids

    # Maximum number of iterations
    if max_iter > 0:
        max_iterations = max_iter;        
    else:
        max_iterations = 0;     

    # Convergence threshold (% of points that changed assignments)
    convergence_log = [100.0];
    if conv_threshold > 0:
        convergence_threshold = conv_threshold;
    else:
        convergence_threshold = 0.001;     
        
    #
    # Non-Adjustable Variables 	
    #
    point_count = 0;            # number of input points
    centr_count = 0;            # initial number of centroids
    centr_final_count = 0;      # final number of centroids
    done = False;               # loop control variable
    gfit = 0;                   # goodness of fit measure

    info( 'Started K-means clustering with parameters:')

    #   
    # Validate all parameters
    #
    
    # Validate: src_relation
    try:
        plpy.execute( "SELECT * FROM " + src_relation + " LIMIT 1")
    except:
        plpy.error( "source relation %s does not exist" % src_relation )
    info( ' * src_relation = %s' % src_relation)

    # Validate: src_col_data
    try:
        plpy.execute( "SELECT " + src_col_data + " FROM " + src_relation + " LIMIT 1")
    except:
        plpy.error( "data column %s not found in relation %s" 
                    % (src_col_data, src_relation) )
    info( ' * src_col_data = %s' % src_col_data)

    # Validate: src_col_id
    try:
        plpy.execute( "SELECT " + src_col_id + " FROM " + src_relation + " LIMIT 1")
        has_pid = 1
        info( ' * src_col_id = %s' % (src_col_id) )
    except:
        has_pid = 0
        info( ' * src_col_id = (not found, will auto-generate)')
        
    # Validate: init_cset_rel and init_cset_col
    if (init_cset_rel is not None) and (init_cset_col is not None):
        try:
            plpy.execute( "SELECT * FROM " + init_cset_rel + " LIMIT 1")
        except:
            plpy.error( "relation %s does not exist" % init_cset_rel )
        info( ' * init_cset_rel = %s' % init_cset_rel)
        try:
            rv = plpy.execute( "SELECT count(" + init_cset_col + ") FROM " + init_cset_rel)
        except:
            plpy.error( "column %s not found in relation %s" 
                        % (init_cset_col, init_cset_rel) )
        if rv[0]['count'] > 0:
            cset_count = rv[0]['count']        
            k = None;
        else:
            plpy.error( "initial centroid relation (%s) is empty" % init_cset_rel)
        info( ' * init_cset_col = %s' % init_cset_col)  
    # Validate: k & init_method
    elif k > 0:
        info( ' * initial k = %s' % k)
        if init_method is None:
            info( ' * init_method = None (default: random)')        
            init_method = 'kmeans++';
        elif init_method.lower() == 'kmeans++':
            info( ' * init_method = kmeans++')
            init_method = 'kmeans++';
        else:
            info( ' * init_method = Unknown (default: random)')
            init_method = 'random';
        cset_count = None;
    # Otherwise
    elif k <= 0:
        plpy.error( "k (" + str(k) + ") must be a positive integer")
    else:        
        plpy.error( "either k or initial centroid set must be specified")

    # Validate: k/cset VS data_points
    rv = plpy.execute( "SELECT count(*) as cnt FROM " + src_relation);
    point_count = rv[0]['cnt'];
    if (point_count == 0):
        plpy.error( "source relation is empty (" + src_relation + ")");
    elif (cset_count > 0 and point_count < cset_count):
        plpy.error( "there is more initial centroids (%s) than data points (%s)" % (cset_count, point_count));
    elif (k > 0 and point_count < k):
        plpy.error( "more initial centroids specified () than data points" % (k, point_count));
        
    # Validate: gof
    if (gof == True):
        info( ' * gof = %s (GOF test on)' % str(gof));
    elif (gof == False):
        info( ' * gof = %s (GOF test off)' % str(gof));
    else:
        plpy.error( 'incorrect value for gof: ' + str(gof) );
        
    # Validate: output_points
    if overwrite == True:
        __run_quietly( 'DROP TABLE IF EXISTS ' + output_points + ' CASCADE')
    try:
        sql = "CREATE TABLE " + output_points + " (" + \
              " pid BIGINT," + \
              " coords " + madlib_schema + ".SVEC," + \
              " cid INT )";
        __run_quietly( sql)
    except:
        plpy.error( "output table %s already exists\n" % output_points )
    info( ' * output_points = %s' % output_points)

    # Validate: output_centroids
    if overwrite == True:
        __run_quietly( 'DROP TABLE IF EXISTS ' + output_centroids + ' CASCADE')
    try:
        sql = "CREATE TABLE " + output_centroids + " (" \
              + " cid INT," \
              + " coords " + madlib_schema + ".SVEC )" 
        __run_quietly( sql)
    except:
        if overwrite == True:
            __run_quietly( 'DROP TABLE ' + output_centroids + ' CASCADE')
            __run_quietly( sql)        
        else:
            plpy.error( "output table %s already exists\n" % output_centroids )
    info( ' * output_centroids = %s' % output_centroids)

    # Sampling setup
    #if sample_pct > 0.0 and sample_pct < 100.0:
    #    sample_size = int( round( (sample_pct/100.0) * point_count));        
    #else:
    #    sample_size = point_count; 

    # Create a view with PID and COORDS columns
    src_view = 'tmp_madlib_view_' + str(os.getpid());
    src_type = madlib_schema + '.svec';
    # Depending on existence of source ID column
    if (has_pid == 0):
        # Generate a new one        
        sql =   "CREATE VIEW " + src_view \
                + " AS SELECT row_number() over() AS pid, " \
                + src_col_data +  "::" + src_type + " AS coords" \
                + " FROM " + src_relation 
        __run_quietly( sql)
    else:
        # Use the current one
        sql =   "CREATE VIEW " + src_view \
                + " AS SELECT " + src_col_id + " as pid, " \
                + src_col_data +  "::" + src_type + " AS coords" \
                + " FROM " + src_relation
        __run_quietly( sql)
                	
	# Prepare the data set
    __run_quietly( 'DROP TABLE IF EXISTS TempTable0');	    
    sql = '''
        CREATE TEMP TABLE TempTable0(
            pid BIGINT, 
            coords ''' + madlib_schema + '''.SVEC, 
            cid INTEGER
        )
    ''';
    __run_quietly( sql);
    sql = '''
        INSERT INTO TempTable0 
        SELECT pid, coords, 0 FROM ''' + src_view;
    plpy.execute( sql);	    
    # Find the number of dimensions
    sql = 'SELECT ' + madlib_schema + '.svec_dimension(coords) as dim FROM ' \
            + src_view + ' LIMIT 1';
    rv = plpy.execute( sql);	    
    info( 'Input data points: ' + str(point_count) + ' (' \
            + str(rv[0]['dim']) + ' dimensions)');

    #if (sample_size > 0 and sample_size < point_count):
    #    info( 'Using sample data set for analysis... (' + str(sample_size) + ' out of ' + str(point_count) + ' points)');
    #    result_analysis = 'analysis based on a sample (' + str(sample_size) + ' out of ' + str(point_count) + ' points)'
    #    sql = '''
    #        INSERT INTO TempTable0 
    #        SELECT pid, coords, 0 FROM ''' + src_view + ''' 
    #        ORDER BY random() 
    #        LIMIT ''' + str( sample_size);
    #else:
    #    info( 'Using full data set for analysis (' + str(point_count) + ' points)');
    #    result_analysis = 'analysis based on full data set (' + str(point_count) + ' points)'
    #    sql = '''
    #        INSERT INTO TempTable0 
    #        SELECT pid, coords, 0 FROM ''' + src_view;
    #plpy.execute( sql);	    

    # Initialize centroids
    if cset_count > 0:
        info( 'Input centroids: %s provided in %s' % (cset_count, init_cset_rel));
        sql = 'INSERT INTO ' + output_centroids + ' (cid, coords) ' \
              + ' SELECT row_number() OVER () AS cid, ' + init_cset_col \
              + ' AS coords FROM ' + init_cset_rel
        plpy.execute( sql); 
        centr_count = cset_count;
        
    elif k > 0 and init_method == 'kmeans++':        
        start = datetime.datetime.now();
        centr_count = __init_plusplus( madlib_schema, src_view, k, point_count);
        time_sec = (datetime.datetime.now() - start).seconds
        info( 'Input centroids: %s seeded using %s method (duration: %s sec)' % (centr_count, init_method, str(time_sec)));
        
    elif k > 0 and init_method == 'random':
        start = datetime.datetime.now();
        centr_count = __init_random( madlib_schema, src_view, k, point_count);
        time_sec = (datetime.datetime.now() - start).seconds
        info( 'Input centroids: %s seeded using %s method (duration: %s sec)' % (centr_count, init_method, str(time_sec)));
        
    else:
        error( 'Something went wrong, check: cset_count=%s, k=%s, init_method=%s' % (cset_count, k, init_method));        
	
    # Main Loop
    i = 0;
    while (done == False):	

        start = datetime.datetime.now();

        # Loop index
        i = i + 1;        
           
        # Create a temporary array of current cetroids
        __run_quietly( 'DROP TABLE IF EXISTS ArrayOfCentroids');	
        sql = '''
            CREATE TEMP TABLE ArrayOfCentroids AS
            SELECT array( 
                SELECT coords 
                FROM
                    (SELECT x FROM generate_series( 1, ''' + str(centr_count) + ''') x) x 
                    LEFT OUTER JOIN ''' + output_centroids + ''' c
                    ON x.x = c.cid 
                ORDER BY x
                LIMIT ''' + str(centr_count) + '''
            ) as arr
        ''';
        __run_quietly( sql);	    
		
		# Create new temp table (i)
        __run_quietly( 'DROP TABLE IF EXISTS TempTable' + str(i));
        __run_quietly( 'CREATE TEMP TABLE TempTable' + str(i) + '(like TempTable' + str(i-1) + ')');

		# For each point assign the closest centroid
        sql = '''
            INSERT INTO TempTable''' + str(i) + '''	
            SELECT
                p.pid, 
                p.coords, 
                ''' + madlib_schema + '''.internal_kmeans_closest_cid( p.coords, arr.arr) as cid 
            FROM 
                TempTable''' + str(i-1) + ''' p CROSS JOIN ArrayOfCentroids arr
        ''';
        plpy.execute( sql);	    

        # Refresh the Centroids table based on current assignments
        plpy.execute( 'TRUNCATE TABLE ' + output_centroids);
        sql = '''
            INSERT INTO ''' + output_centroids + '''
            SELECT t.cid, t.coords 
            FROM 
            (
                SELECT 
                    ''' + madlib_schema + '''.avg( coords) AS coords
                    , cid AS cid 
                FROM TempTable''' + str(i) + ''' 
                GROUP BY cid
            ) AS t''';
        plpy.execute( sql);	    
        
        # Calculate the number of points that changed the assignment
        sql = '''
            SELECT count(*) as sum 
            FROM
                (
                SELECT t1.pid, t1.cid FROM TempTable''' + str(i) + ''' t1
                EXCEPT
                SELECT t2.pid, t2.cid FROM TempTable''' + str(i-1) + ''' t2
                ) t 
        ''';
        rv = plpy.execute( sql);
        time_sec = (datetime.datetime.now() - start).seconds
        info( '... Iteration %s: updated %s points (duration: %s sec)' \
                % (str(i), str(rv[0]['sum']), str(time_sec)));
        
		# Drop previous temp table (i-1)
        __run_quietly( 'DROP TABLE TempTable' + str(i-1));
        
        # Add it to the tracking variable
        if (i>1): convergence_log.append( rv[0]['sum'] / (point_count * 1.0));

        # Exit conditions:
        if (i>3) and (convergence_log[i-4] - convergence_log[i-1] < 0): 
            done = True;
            info( 'Exit condition: fraction of reassigned nodes has stabilized around ' \
                    + str(( convergence_log[i-4] + convergence_log[i-3] + convergence_log[i-2] + convergence_log[i-1]) / 4.0));            
        elif (convergence_log[i-1] < convergence_threshold):
            done = True;
            info( 'Exit condition: fraction of reassigned nodes is smaller than: ' + str(convergence_threshold));
        elif (i == max_iterations):
            done = True;
            info( 'Exit condition: reached maximum number of iterations = ' + str(max_iterations));
        
    # Main Loop - END
            
    # Expand to all points if executed on a sample
    #if ( sample_size < point_count):
    #    info( 'Expanding cluster assignment to all points...');
    #    sql = '''
    #        INSERT INTO ''' + output_points + '''	
    #        SELECT
    #            p.pid, 
    #            p.coords, 
    #            ''' + madlib_schema + '''.internal_kmeans_closest_cid( p.coords, arr.arr) as cid 
    #        FROM 
    #            ''' + src_view + ''' p CROSS JOIN ArrayOfCentroids arr
    #    ''';
    # Fill the output table
    #else:
    info( 'Writing output table: ' + output_points + '...');
    sql = '''
        INSERT INTO ''' + output_points + '''	
        SELECT * FROM TempTable''' + str(i);    
    
    plpy.execute( sql);	  
            
    # Calculate Goodness of fit
    # Defined as = (BaseFit - ModelFit) / BaseFit
    # where: BaseFit = sum( l2norm( p, avg(p)) 
    #        ModelFit = sum( l2nrm( p, c)
    # Previous method:  sum( l2norm(p,c)) / count(*)
    # Suggested by Eugene:  sum( l2norm(p,c)) / max( avg(p) - p)
    if gof == True :
        info( 'Calculating goodness of fit measure...');
        sql = '''
            SELECT ( BaseFit - ModelFit) / BaseFit as gfit
            FROM
                (
                SELECT 
                    sum( ''' + madlib_schema + '''.svec_l2norm( p.coords operator(''' + madlib_schema + '''.-) c.coords)) as ModelFit
                    , sum( ''' + madlib_schema + '''.svec_l2norm( p.coords operator(''' + madlib_schema + '''.-) a.mean)) as BaseFit
                FROM 
                    ''' + output_points + ''' p, ''' + output_centroids + ''' c
                    , (SELECT ''' + madlib_schema + '''.avg(coords) as mean FROM ''' + output_points + ''') a
                WHERE p.cid = c.cid
                ) x
        ''';
        rv = plpy.execute( sql);
        gfit = '%s' % rv[0]['gfit'];
    else:
        gfit = None;
        
    # Count final number of centroids        
    rv = plpy.execute( 'SELECT count(*) as count FROM ' + output_centroids);
    centr_final_count = rv[0]['count'];
        
    # Drop the input view
    rv = plpy.execute( "DROP VIEW " + src_view );  

    # Return output result
    return ( 
        src_relation,   
        point_count, 
        init_cset_rel,
        k,
        init_method,
        i,
        #sample_pct,
        #sample_size, 
        centr_final_count, 
        gfit,
        output_points, 
        output_centroids,
    )
