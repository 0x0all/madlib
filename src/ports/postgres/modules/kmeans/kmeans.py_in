# coding=utf-8

"""
@file kmeans.py_in

@brief k-Means Clustering module

@namespace kmeans
"""

import datetime
import plpy
from math import floor, log, pow
import os

# ----------------------------------------
# K-means global variables
# ----------------------------------------
global verbose, output_points, output_centroids

# ----------------------------------------
# Logging
# ----------------------------------------
def info( msg):
    if verbose: 
        plpy.info( msg)
   
# ----------------------------------------
# Quotes a string to be used as an identifier 
# ----------------------------------------
#def quote_ident(val):
#    return '"' + val.replace('"', '""') + '"'

# ----------------------------------------
# Quotes a string to be used as a literal 
# ----------------------------------------
#def quote_literal(val):
#    return "'" + val.replace("'", "''") + "'"

# ----------------------------------------
# Run SQL in ERROR only message mode
# ----------------------------------------
def __run_quietly(sql):

    # Remember current message level and set it to ERROR
    prev_msg_level = plpy.execute("SELECT setting FROM pg_settings " \
        + " WHERE name='client_min_messages'")[0]['setting']
    plpy.execute("SET client_min_messages = error;")

    # Run...
    rv = plpy.execute(sql)

    # Restrore previous message level
    plpy.execute("SET client_min_messages = %s;" % prev_msg_level)
    
    return rv

# ----------------------------------------
# Centroid initialization using kmeans++
# ----------------------------------------
def __init_plusplus( madlib_schema, src_view, src_col_data, src_col_id, k):
    """
    Creates the initial set of centroids.

    XXX - add parameters info
    """
	
    # Find how many points we need to look through to be 99.9% sure that
    # we have seen a point from each cluster 
    numCentroids = floor( - log( 1 - pow( 0.999, 1/float(k))) * k);

    # Find the 1st centroid
    info( 'Seeding ' + str(k) + ' centroid(s)...');
    sql = '''    
        INSERT INTO ''' + output_centroids + ''' (cid, coords) 
        SELECT 1, coords::''' + madlib_schema + '''.SVEC 
        FROM ''' + src_view + '''
        ORDER BY random() DESC 
        LIMIT 1
        ''';
    plpy.execute( sql);

    # Find remaining centroids
    i = 1
    while (i < k):
        i = i + 1;
        sql = '''
            INSERT INTO ''' + output_centroids + ''' (cid, coords) 
            SELECT ''' + str(i) + ''', pt.coords 
            FROM 
                ''' + src_view + ''' pt,
                ( 
                -- Q1: Find a random point
                -- (that is furthest away from current Centroids)
                --
                SELECT p.pid, min( ''' + madlib_schema + '.svec_l2norm(p.coords operator(''' + madlib_schema + '''.-) c.coords)) * (random()^(0.2)) AS distance 
                FROM
                    (SELECT coords, pid FROM ''' + src_view + ''' ORDER BY random() LIMIT ''' + str(numCentroids) + ''') AS p -- K random points
                    CROSS JOIN 
                    (SELECT coords FROM ''' + output_centroids + ''') AS c -- current centroids
                GROUP BY p.pid ORDER BY distance DESC LIMIT 1
                ) AS pc 
                --
                -- Q1: end
                --
            WHERE pc.pid = pt.pid
        ''';
        plpy.execute( sql);

    # Return # of created centroids
    return i
    
# ----------------------------------------
# Function to run the k-means algorithm
# ----------------------------------------
def kmeans( madlib_schema
            , src_relation, src_col_data, src_col_id
            , k, gof
            , out_points, out_centroids, overwrite
            , p_verbose):
            
    """
    Executes the k-means clustering algorithm.
    
    XXX - add parameters info
    """

    # Global variables
    global verbose, output_points, output_centroids
    verbose = p_verbose
    output_points = out_points
    output_centroids = out_centroids
    
    #
    # Adjustable Variables 	
    #
    sampling = 1;               # If set to 1 core sampling is on otherwise 
                                # algorithm will be executed on the full data set
    sampling_size = 100;        # Make the sample size sufficient to have 
                                # at least 'sampling_size' elements from each 
                                # cluster with p = .999
    max_sample_size = 10000000; # maximum sample size 
    change_pct_limit = 0.001;   # % of points to change assigment
    max_iterations = 20;        # Maximum number of allowed iterations 

    #
    # Non-Adjustable Variables 	
    #
    point_count = 0;        # number of input points
    centr_count = 0;        # number of initial centroids
    centr_final_count = 0;  # number of final centroids
    change_pct = [100.0];   # error array
    sample_size = 0;        # sample size - auto generated
    used_sample_size = None;# used sample size 
    expand = 0;             # set to 1 if the k-means was executed on a sample set
    done = 0;               # loop control variable
    gfit = '';              # goodness of fit description

    info( 'Started K-means clustering:')

    #   
    # Validate all parameters
    #
    
    # Validate: src_relation
    try:
        plpy.execute( "SELECT * FROM " + src_relation + " LIMIT 1")
    except:
        plpy.error( "source relation (%s) does not exist" % src_relation )
    info( ' * src_relation = %s' % src_relation)

    # Validate: src_col_data
    try:
        plpy.execute( "SELECT " + src_col_data + " FROM " + src_relation + " LIMIT 1")
    except:
        plpy.error( "data column (%s) not found in relation %s" 
                    % (src_col_data, src_relation) )
    info( ' * src_col_data = %s' % src_col_data)

    # Validate: src_col_id
    try:
        plpy.execute( "SELECT " + src_col_id + " FROM " + src_relation + " LIMIT 1")
        has_pid = 1
        info( ' * src_col_id = %s' % (src_col_id) )
    except:
        has_pid = 0
        info( ' * src_col_id = (not found, will auto-generate)')

    # Validate: k (positive integer)
    if not(k>0):
        plpy.error( "number of centroids must be a positive integer (" + str(k) + ")")
    else:
        info( ' * k = %s (number of centroids)' % k)

    # # Validate: k (and number of input rows)
    rv = plpy.execute( "SELECT count(*) as cnt FROM " + src_relation);
    point_count = rv[0]['cnt'];
    if (point_count == 0):
        plpy.error( "source relation is empty (" + src_relation + ")");
    elif (point_count < k):
        plpy.error( "input table has less points (" + str(point_count) + ") "
                    + "than requested number of clusters (" + str(k) + ")");
        
    # Validate: gof
    if (gof == True):
        info( ' * gof = %s (GOF test on)' % str(gof));
    elif (gof == False):
        info( ' * gof = %s (GOF test off)' % str(gof));
    else:
        plpy.error( 'incorrect value for gof: ' + str(gof) );
        
    # Validate: output_points
    if overwrite == True:
        __run_quietly( 'DROP TABLE IF EXISTS ' + output_points + ' CASCADE')
    try:
        sql = "CREATE TABLE " + output_points + " (" + \
              " pid BIGINT," + \
              " coords " + madlib_schema + ".SVEC," + \
              " cid INT )";
        __run_quietly( sql)
    except:
        plpy.error( "output table %s already exists\n" % output_points )
    info( ' * output_points = %s' % output_points)

    # Validate: output_centroids
    if overwrite == True:
        __run_quietly( 'DROP TABLE IF EXISTS ' + output_centroids + ' CASCADE')
    try:
        sql = "CREATE TABLE " + output_centroids + " (" \
              + " cid INT," \
              + " coords " + madlib_schema + ".SVEC )" 
        __run_quietly( sql)
    except:
        if overwrite == True:
            __run_quietly( 'DROP TABLE ' + output_centroids + ' CASCADE')
            __run_quietly( sql)        
        else:
            plpy.error( "output table %s already exists\n" % output_centroids )
    info( ' * output_centroids = %s' % output_centroids)

    # Create a view with PID and COORDS columns
    src_view = 'tmp_madlib_view_' + str(os.getpid());
    src_type = madlib_schema + '.svec';
    # Depending on existence of source ID column
    if (has_pid == 0):
        # Generate a new one        
        sql =   "CREATE VIEW " + src_view \
                + " AS SELECT row_number() over() AS pid, " \
                + src_col_data +  "::" + src_type + " AS coords" \
                + " FROM " + src_relation 
        __run_quietly( sql)
    else:
        # Use the current one
        sql =   "CREATE VIEW " + src_view \
                + " AS SELECT " + src_col_id + " as pid, " \
                + src_col_data +  "::" + src_type + " AS coords" \
                + " FROM " + src_relation
        __run_quietly( sql)
    
    # Initialize centroids
    centr_count = __init_plusplus( madlib_schema, src_view, src_col_data, src_col_id, k);
    if (centr_count != k):
        info( 'Requested %d centroids, but %d have been seeded.' % (k, centr_count) );
            
    # Calculate the sample size
    sample_size = min( int( sampling_size * floor( - log( 1 - pow( 0.999, 1/float(k))) * k)), max_sample_size);
	
	# Prepare either the sample or the full data set
    __run_quietly( 'DROP TABLE IF EXISTS TempTable0');	    
    sql = '''
        CREATE TEMP TABLE TempTable0(
            pid BIGINT, 
            coords ''' + madlib_schema + '''.SVEC, 
            cid INTEGER
        )
    ''';
    __run_quietly( sql);
    if (sampling == 1 and sample_size < point_count):
        info( 'Using sample data set for analysis... (' + str(sample_size) + ' out of ' + str(point_count) + ' points)');
        result_analysis = 'analysis based on a sample (' + str(sample_size) + ' out of ' + str(point_count) + ' points)'
        sql = '''
            INSERT INTO TempTable0 
            SELECT pid, coords, 0 FROM ''' + src_view + ''' 
            ORDER BY random() 
            LIMIT ''' + str( sample_size);
        expand = 1;
        used_sample_size = sample_size;
    else:
        info( 'Using full data set for analysis (' + str(point_count) + ' points)');
        result_analysis = 'analysis based on full data set (' + str(point_count) + ' points)'
        sql = '''
            INSERT INTO TempTable0 
            SELECT pid, coords, 0 FROM ''' + src_view;
        expand = 0;
        used_sample_size = 0;
    plpy.execute( sql);	    
	
    # Main Loop
    i = 0;
    while (done == 0):	

        # Loop index
        i = i + 1;        
        info( '...Iteration ' + str(i));
           
        # Create a temporary array of current cetroids
        __run_quietly( 'DROP TABLE IF EXISTS ArrayOfCentroids');	
        sql = '''
            CREATE TEMP TABLE ArrayOfCentroids AS
            SELECT array( 
                SELECT coords 
                FROM
                    (SELECT x FROM generate_series( 1, ''' + str(k) + ''') x) x 
                    LEFT OUTER JOIN ''' + output_centroids + ''' c
                    ON x.x = c.cid 
                ORDER BY x
                LIMIT ''' + str(k) + '''
            ) as arr
        ''';
        __run_quietly( sql);	    
		
		# Create new temp table (i)
        __run_quietly( 'DROP TABLE IF EXISTS TempTable' + str(i));
        __run_quietly( 'CREATE TEMP TABLE TempTable' + str(i) + '(like TempTable' + str(i-1) + ')');

		# For each point assign the closest centroid
        sql = '''
            INSERT INTO TempTable''' + str(i) + '''	
            SELECT
                p.pid, 
                p.coords, 
                ''' + madlib_schema + '''.internal_kmeans_closest_cid( p.coords, arr.arr) as cid 
            FROM 
                TempTable''' + str(i-1) + ''' p CROSS JOIN ArrayOfCentroids arr
        ''';
        plpy.execute( sql);	    

        # Refresh the Centroids table based on current assignments
        plpy.execute( 'TRUNCATE TABLE ' + output_centroids);
        sql = '''
            INSERT INTO ''' + output_centroids + '''
            SELECT t.cid, t.coords 
            FROM 
            (
                SELECT 
                    ''' + madlib_schema + '''.avg( coords) AS coords
                    , cid AS cid 
                FROM TempTable''' + str(i) + ''' 
                GROUP BY cid
            ) AS t''';
        plpy.execute( sql);	    
        
        # Calculate the number of points that changed the assignment
        sql = '''
            SELECT SUM( ABS( t1.e - t2.e))::float as sum 
            FROM
                (SELECT count( t1.pid) AS e, t1.cid FROM TempTable''' + str(i) + ''' t1 GROUP BY t1.cid) AS t1 
                JOIN 
                (SELECT count( t2.pid) AS e, t2.cid FROM TempTable''' + str(i-1) + ''' t2 GROUP BY t2.cid) AS t2 
                ON t1.cid = t2.cid
        ''';
        rv = plpy.execute( sql);
        
		# Drop previous temp table (i-1)
        __run_quietly( 'DROP TABLE TempTable' + str(i-1));
        
        # Add it to the tracking variable
        if (i>1): change_pct.append( rv[0]['sum'] / point_count);

        # Exit conditions:
        if (i>3) and (change_pct[i-4] - change_pct[i-1] < 0): 
            done = 1;
            info( 'Exit condition: fraction of reassigned nodes has stabilized around ' + str(( change_pct[i-4] + change_pct[i-3] + change_pct[i-2] + change_pct[i-1]) / 4.0));            
        elif (change_pct[i-1] < change_pct_limit):
            done = 1;
            info( 'Exit condition: fraction of reassigned nodes is smaller than the limit: ' + str(change_pct_limit));
        elif (i==max_iterations):
            done = 1;
            info( 'Exit condition: reached the maximum number of allowed iterations: ' + str(max_iterations));
        
    # Main Loop - END
            
    # Expand to all points if executed on a sample
    if ( expand == 1):
        info( 'Expanding cluster assignment to all points...');
        sql = '''
            INSERT INTO ''' + output_points + '''	
            SELECT
                p.pid, 
                p.coords, 
                ''' + madlib_schema + '''.internal_kmeans_closest_cid( p.coords, arr.arr) as cid 
            FROM 
                ''' + src_view + ''' p CROSS JOIN ArrayOfCentroids arr
        ''';
    # Fill the output table
    else:
        info( 'Writing final output table...');
        sql = '''
            INSERT INTO ''' + output_points + '''	
            SELECT * FROM TempTable''' + str(i);    
    
    plpy.execute( sql);	  
            
    # Calculate Goodness of fit
    # This version uses sum( l2norm())/count(*)
    # It would be better to do sum( l2norm(p,c)) / max( avg(points) - point)
    if gof == True :
        info( 'Calculating goodness of fit...');
        sql = '''
            SELECT sum( ''' + madlib_schema + '''.svec_l2norm(p.coords operator(''' + madlib_schema + '''.-) c.coords)) / count(*) as gfit
            FROM ''' + output_points + ''' p, ''' + output_centroids + ''' c
            WHERE p.cid = c.cid
        ''';
        rv = plpy.execute( sql);
        gfit = '%s' % rv[0]['gfit'];
    else:
        gfit = None;
        
    # Count final number of centroids        
    rv = plpy.execute( 'SELECT count(*) as count FROM ' + output_centroids);
    centr_final_count = rv[0]['count'];
        
    # Cleanup
    # if (has_pid == 0):
    # Drop the input view
    rv = plpy.execute( "DROP VIEW " + src_view );  

    # Return output result
    return ( 
        src_relation,
        point_count, 
        centr_final_count, 
        (used_sample_size*100.0)/point_count, 
        gfit,
        output_points, 
        output_centroids,
    )
