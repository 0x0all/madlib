/* ----------------------------------------------------------------------- *//** 
 *
 * @file bayes.sql_in
 *
 * @brief SQL functions for naive Bayes
 * @date   January 2011
 *
 * @sa For a brief introduction to Naive Bayes Classification, see the module
 *     description \ref grp_bayes.
 *
 *//* ----------------------------------------------------------------------- */

m4_include(`SQLCommon.m4')

/**
@addtogroup grp_bayes

@about

This is a Naive Bayes classification module with user-defined smoothing factor 
(the default is "Laplacian smoothing").

A Naive Bayes classifier computes the following formula:
\f[
    \text{classify}(a_1, ..., a_n)
    =   \arg\max_c \left\{
            P(C = c) \cdot \prod_{i=1}^n P(A_i = a_i \mid C = c)
        \right\}
\f]
where \f$ c \f$ ranges over all classes in the training data and probabilites
are estimated with relative frequencies from the training set.

There are different ways to estimate the feature probabilities
\f$ P(A_i = a \mid C = c) \f$.  The maximum likelihood estimate takes the
relative frequencies. That is:
\f[
    P(A_i = a \mid C = c) = \frac{\#(c,i,a)}{\#c}
\f]
where
- \f$ \#(c,i,a) \f$ denotes the # of training samples where attribute \f$ i \f$
  is \f$ a \f$ and class is \f$ c \f$
- \f$ \#c \f$ denotes the # of training samples where class is \f$ c \f$.

Since the maximum likelihood sometimes results in estimates of "0", you might
want to use a "smoothed" estimate. To do this, you add a number of "virtual"
samples and make the assumption that these samples are evenly distributed among
the values assumed by attribute \f$ i \f$ (that is, the set of all values
observed for attribute \f$ a \f$ for any class):

\f[
    P(A_i = a \mid C = c) = \frac{\#(c,i,a) + s}{\#c + s \cdot \#i}
\f]
where
- \f$ \#i \f$ denotes the # of distinct values for attribute \f$ i \f$ (for all
  classes)
- \f$ s \geq 0 \f$ denotes the smoothing factor.

The case \f$ s = 1 \f$ is known as "Laplace smoothing". The case \f$ s = 0 \f$
trivially reduces to maximum-likelihood estimates.

@usage

-   <b>Input</b>\n\n
    The <b>training data</b> is expected to be of the following form:\n\n 
    <tt>{TABLE|VIEW} <em>trainingSource</em> (\n
        &nbsp;&nbsp;&nbsp;...\n
        &nbsp;&nbsp;&nbsp;<em>trainingClassColumn</em> INTEGER\n 
        &nbsp;&nbsp;&nbsp;<em>trainingAttrColumn</em> INTEGER[]\n
        &nbsp;&nbsp;&nbsp;...\n
    )</tt>\n\n
    The <b>data to classify</b> is expected to be of the following form:\n\n
    <tt>{TABLE|VIEW} <em>classifySource</em> (\n
        &nbsp;&nbsp;&nbsp;...\n
        &nbsp;&nbsp;&nbsp;<em>classifyKeyColumn</em> ANYTYPE\n
        &nbsp;&nbsp;&nbsp;<em>classifyAttrColumn</em> INTEGER[]\n
        &nbsp;&nbsp;&nbsp;...\n
    )</tt>\n\n
    
-   <b>Option #1 (with pre-computation step)</b>
    \n\n
    -# For faster classification in the following steps you can run the 
    <tt>\ref create_nb_prepared_data_tables(
    '<em>trainingSource</em>', 
    '<em>trainingClassColumn</em>', 
    '<em>trainingAttrColumn</em>',
    <em>numAttrs</em>, 
    '<em>featureProbsName</em>', 
    '<em>classPriorsName</em>')</tt>
    function to create tables of pre-computed feature probabilities 
    and class priors of the following structure:
    \n\n
    <tt>TABLE <em>featureProbsName</em> (\n
        &nbsp;&nbsp;&nbsp;class INTEGER,\n
        &nbsp;&nbsp;&nbsp;attr INTEGER,\n 
        &nbsp;&nbsp;&nbsp;value INTEGER,\n 
        &nbsp;&nbsp;&nbsp;cnt INTEGER,\n 
        &nbsp;&nbsp;&nbsp;attr_cnt INTEGER\n
    )</tt>\n\n
    <tt>TABLE <em>classPriorsName</em> (\n
        &nbsp;&nbsp;&nbsp;class INTEGER,\n
        &nbsp;&nbsp;&nbsp;class_cnt INTEGER,\n
        &nbsp;&nbsp;&nbsp;all_cnt INTEGER\n
    )</tt>
    \n\n
    -# To create the Naive-Bayes classification view for the source data (based 
    on the pre-computed <em>featureProbsName</em> and <em>classPriorsName</em> 
    tables) run
    <tt>\ref create_nb_classify_view (
    '<em>featureProbsName</em>',
    '<em>classPriorsName</em>',
    '<em>classifySource</em>',
    '<em>classifyKeyColumn</em>',
    '<em>classifyAttrColumn</em>',
    <em>numAttrs</em>,
    '<em>destView</em>')</tt>. 
    The output view (<em>destView</em>) will contain one row for every value of 
    <tt><em>classifyKeyColumn</em></tt> in <tt><em>classifySource</em></tt> table:\n\n
    <tt>VIEW <em>destView</em> (\n
        &nbsp;&nbsp;&nbsp;key ANYTYPE,\n
        &nbsp;&nbsp;&nbsp;nb_classification INTEGER\n
    )</tt>
    \n\n
    -# To create the view with classification probabilities 
    (based on the pre-computed <em>featureProbsName</em> 
    and <em>classPriorsName</em> tables) run
    <tt>\ref create_nb_probs_view (
    '<em>featureProbsName</em>',
    '<em>classPriorsName</em>',
    '<em>classifySource</em>',
    '<em>classifyKeyColumn</em>',
    '<em>classifyAttrColumn</em>',
    <em>numAttrs</em>,
    '<em>destView</em>')</tt>. 
    The output view (<em>destView</em>) will contain one row for every (key,class) 
    pair:\n\n
    <tt>VIEW <em>destView</em> (\n
        &nbsp;&nbsp;&nbsp;key ANYTYPE,\n
        &nbsp;&nbsp;&nbsp;class INTEGER,\n
        &nbsp;&nbsp;&nbsp;nb_prob FLOAT\n
    )</tt>\n
       
-   <b>Option #2 (w/o pre-computation step)</b>
    \n\n
    -# This option is does not include the creating of the pre-computed 
    feature probabilities and class priors tables.
    \n\n
    -# To create the Naive-Bayes classification view for the source data run
    <tt>\ref create_nb_classify_view (
    '<em>trainingSource</em>',
    '<em>trainingClassColumn</em>',
    '<em>trainingAttrColumn</em>',
    '<em>classifySource</em>',
    '<em>classifyKeyColumn</em>',
    '<em>classifyAttrColumn</em>',
    <em>numAttrs</em>,
    '<em>destView</em>')</tt>. 
    The output view (<em>destView</em>) will contain one row for every value of 
    <tt><em>classifyKeyColumn</em></tt> in <tt><em>classifySource</em></tt> table:\n\n
    <tt>VIEW <em>destView</em> (\n
        &nbsp;&nbsp;&nbsp;key ANYTYPE,\n
        &nbsp;&nbsp;&nbsp;nb_classification INTEGER\n
    )</tt>
    \n\n
    -# To create the view with the classification probabilities run
    <tt>\ref create_nb_probs_view (
    '<em>trainingSource</em>',
    '<em>trainingClassColumn</em>',
    '<em>trainingAttrColumn</em>',
    '<em>classifySource</em>',
    '<em>classifyKeyColumn</em>',
    '<em>classifyAttrColumn</em>',
    <em>numAttrs</em>,
    '<em>destView</em>')</tt>. 
    The output view (<em>destView</em>) will contain one row for every (key,class) 
    pair:\n\n
    <tt>VIEW <em>destView</em> (\n
        &nbsp;&nbsp;&nbsp;key ANYTYPE,\n
        &nbsp;&nbsp;&nbsp;class INTEGER,\n
        &nbsp;&nbsp;&nbsp;nb_prob FLOAT\n
    )</tt>\n

@examp

The following is an extremely simplified example of the above option #1 which
can by verified by hand.

-#  Create the training and the to-be-classified data sets:
\verbatim
sql> CREATE TABLE training (id INT, class INT, attributes INT[]);
sql> COPY training FROM STDIN WITH DELIMITER '|';
    1 |     1 | {1,2,3}
    2 |     1 | {1,2,1}
    3 |     1 | {1,4,3}
    4 |     2 | {1,2,2}
    5 |     2 | {0,2,2}
    6 |     2 | {0,1,3}
\.
sql> CREATE TABLE toclassify (id INT, attributes INT[]);
sql> COPY toclassify FROM STDIN WITH DELIMITER '|';
  1 | {0,2,1}
  2 | {1,2,3}
\.
\endverbatim
\n
-#  Make sure they look good:
\verbatim
sql> SELECT * FROM training;
 id | class | attributes 
----+-------+------------
  1 |     1 | {1,2,3}
  2 |     1 | {1,2,1}
  3 |     1 | {1,4,3}
  4 |     2 | {1,2,2}
  5 |     2 | {0,2,2}
  6 |     2 | {0,1,3}
(6 rows)

sql> select * from toclassify;
 id | attributes 
----+------------
  1 | {0,2,1}
  2 | {1,2,3}
(2 rows)
\endverbatim
\n
-# Precompute feature probabilities and class priors
\verbatim
sql> SELECT madlib.create_nb_prepared_data_tables( 
'training', 'class', 'attributes', 3, 'nb_feature_probs', 'nb_class_priors');
\endverbatim
\n
-# Optionally check the contents of the precomputed tables:
\verbatim
sql> SELECT * FROM nb_class_priors;
 class | class_cnt | all_cnt 
-------+-----------+---------
     1 |         3 |       6
     2 |         3 |       6
(2 rows)

sql> SELECT * FROM nb_feature_probs;
 class | attr | value | cnt | attr_cnt 
-------+------+-------+-----+----------
     1 |    1 |     0 |   0 |        2
     1 |    1 |     1 |   3 |        2
     1 |    2 |     1 |   0 |        3
     1 |    2 |     2 |   2 |        3
...
\endverbatim
\n
-# Create the view with Naive Bayes classification and check the results:
\verbatim
sql> SELECT madlib.create_nb_classify_view (
'nb_feature_probs', 'nb_class_priors', 'toclassify', 'id', 'attributes', 3, 'nb_classify_view_fast');

sql> SELECT * FROM nb_classify_view_fast;
 key | nb_classification 
-----+-------------------
   1 | {2}
   2 | {1}
(2 rows)
\endverbatim
\n
-# Look at the probabilities for each class (note that we use "Laplacian smoothing"):
\verbatim
sql> SELECT madlib.create_nb_probs_view (
'nb_feature_probs', 'nb_class_priors', 'toclassify', 'id', 'attributes', 3, 'nb_probs_view_fast');

sql> SELECT * FROM nb_probs_view_fast;
 key | class | nb_prob 
-----+-------+---------
   1 |     1 |     0.4
   1 |     2 |     0.6
   2 |     1 |    0.75
   2 |     2 |    0.25
(4 rows)
\endverbatim

@sa File bayes.sql_in documenting the SQL functions.

@internal
@sa namespace bayes (documenting the implementation in Python)
@endinternal

@literature

-   [1] Tom Mitchell: Machine Learning, McGraw Hill, 1997. Book chapter
    <em>Generativ and Discriminative Classifiers: Naive Bayes and Logistic
    Regression</em> available at: http://www.cs.cmu.edu/~tom/NewChapters.html

-   [2] Wikipedia, Naive Bayes classifier,
    http://en.wikipedia.org/wiki/Naive_Bayes_classifier
*/

-- Begin of argmax definition

CREATE TYPE MADLIB_SCHEMA.ARGS_AND_VALUE_DOUBLE AS (
    args INTEGER[],
    value DOUBLE PRECISION
);

CREATE FUNCTION MADLIB_SCHEMA.argmax_transition(
    oldmax MADLIB_SCHEMA.ARGS_AND_VALUE_DOUBLE,
    newkey INTEGER,
    newvalue DOUBLE PRECISION)
RETURNS MADLIB_SCHEMA.ARGS_AND_VALUE_DOUBLE AS
$$
    SELECT CASE WHEN $3 < $1.value OR $2 IS NULL OR ($3 IS NULL AND NOT $1.value IS NULL) THEN $1
                WHEN $3 = $1.value OR ($3 IS NULL AND $1.value IS NULL AND NOT $1.args IS NULL)
                    THEN ($1.args || $2, $3)::MADLIB_SCHEMA.ARGS_AND_VALUE_DOUBLE
                ELSE (array[$2], $3)::MADLIB_SCHEMA.ARGS_AND_VALUE_DOUBLE
           END
$$
LANGUAGE sql IMMUTABLE;

CREATE FUNCTION MADLIB_SCHEMA.argmax_combine(
    max1 MADLIB_SCHEMA.ARGS_AND_VALUE_DOUBLE,
    max2 MADLIB_SCHEMA.ARGS_AND_VALUE_DOUBLE)
RETURNS MADLIB_SCHEMA.ARGS_AND_VALUE_DOUBLE AS
$$
    -- If SQL guaranteed short-circuit evaluation, the following could become
    -- shorter. Unfortunately, this is not the case.
    -- Section 6.3.3.3 of ISO/IEC 9075-1:2008 Framework (SQL/Framework):
    --
    --  "However, it is implementation-dependent whether expressions are
    --   actually evaluated left to right, particularly when operands or
    --   operators might cause conditions to be raised or if the results of the
    --   expressions can be determined without completely evaluating all parts
    --   of the expression."
    --
    -- Again, the optimizer does its job hopefully.
    SELECT CASE WHEN $1 IS NULL THEN $2
                WHEN $2 IS NULL THEN $1
                WHEN ($1.value = $2.value) OR ($1.value IS NULL AND $2.value IS NULL)
                    THEN ($1.args || $2.args, $1.value)::MADLIB_SCHEMA.ARGS_AND_VALUE_DOUBLE
                WHEN $1.value IS NULL OR $1.value < $2.value THEN $2
                ELSE $1
           END
$$
LANGUAGE sql IMMUTABLE;

CREATE FUNCTION MADLIB_SCHEMA.argmax_final(
    finalstate MADLIB_SCHEMA.ARGS_AND_VALUE_DOUBLE)
RETURNS INTEGER[] AS
$$
    SELECT $1.args
$$
LANGUAGE sql IMMUTABLE;

/**
 * @internal
 * @brief Argmax: Return the key of the row for which value is maximal
 *
 * The "index set" of the argmax function is of type INTEGER and we range over
 * DOUBLE PRECISION values. It is not required that all keys are distinct.
 *
 * @note
 * argmax should only be used on unsorted data because it will not exploit
 * indices, and its running time is \f$ \Theta(n) \f$.
 *
 * @implementation
 *
 * The implementation is in SQL, with a flavor of functional programming.
 * The hope is that the optimizer does a good job here.
 */
CREATE AGGREGATE MADLIB_SCHEMA.argmax(/*+ key */ INTEGER, /*+ value */ DOUBLE PRECISION) (
    SFUNC=MADLIB_SCHEMA.argmax_transition,
    STYPE=MADLIB_SCHEMA.ARGS_AND_VALUE_DOUBLE,
    m4_ifdef(`GREENPLUM',`prefunc=MADLIB_SCHEMA.argmax_combine,')
    FINALFUNC=MADLIB_SCHEMA.argmax_final
);


/**
 * @brief Precompute all class priors and feature probabilities
 *
 * Feature probabilities are stored in a table with columns
 * (class, attr, value, cnt, attr_cnt)
 *
 * Class priors are stored in a relation with columns
 * (class, class_cnt, all_cnt).
 *
 * @param trainingSource Name of relation containing the training data
 * @param trainingClassColumn Name of class column in training data
 * @param trainingAttrColumn Name of attributes-array column in training data
 * @param numAttrs Number of attributes to use for classification
 * @param featureProbsDestName Name of feature-probabilities table to create
 * @param classPriorsDestName Name of class-priors table to create
 *
 * @internal
 * @sa This function is a wrapper for bayes::create_prepared_data().
 */
CREATE FUNCTION MADLIB_SCHEMA.create_nb_prepared_data_tables(
    "trainingSource" VARCHAR,
    "trainingClassColumn" VARCHAR,
    "trainingAttrColumn" VARCHAR,
    "numAttrs" INTEGER,
    "featureProbsDestName" VARCHAR,
    "classPriorsDestName" VARCHAR)
RETURNS VOID
AS $$PythonFunction(bayes, bayes, create_prepared_data_table)$$
LANGUAGE plpythonu VOLATILE;

/**
 * @brief Create view that contains all keys of the source relation and the
 *        respective naive Bayes classifications
 *
 * @param featureProbsSource Name of table with precomputed feature
 *        probabilities, as created with create_nb_prepared_data_tables()
 * @param classPriorsSource Name of table with precomputed class priors, as
 *        created with create_nb_prepared_data_tables()
 * @param classifySource Name of the relation that contains data to be classified
 * @param classifyKeyColumn Name of column in \em classifySource that can
 *        serve as unique identifier (the key of the source relation)
 * @param classifyAttrColumn Name of attributes-array column in \em classifySource
 * @param numAttrs Number of attributes to use for classification
 * @param destName Name of the view to create
 *
 * @internal
 * @sa This function is a wrapper for bayes::create_classification(). See there
 *     for details.
 */
CREATE FUNCTION MADLIB_SCHEMA.create_nb_classify_view(
    "featureProbsSource" VARCHAR,
    "classPriorsSource" VARCHAR,
    "classifySource" VARCHAR,
    "classifyKeyColumn" VARCHAR,
    "classifyAttrColumn" VARCHAR,
    "numAttrs" INTEGER,
    "destName" VARCHAR)
RETURNS VOID
AS $$PythonFunction(bayes, bayes, create_classification_view)$$
LANGUAGE plpythonu VOLATILE;


/**
 * @brief Create a view with columns <tt>(key, nb_classification)</tt>
 *
 * The created relation will be
 *
 * <tt>{TABLE|VIEW} <em>destName</em> (key, nb_classification)</tt>
 *  
 * where \c nb_classification is an array containing the most likely
 * class(es) of the record in \em classifySource identified by \c key.
 *
 * @param trainingSource
 *        Name of relation containing the training data
 * @param trainingClassColumn
 *        Name of class column in training data
 * @param trainingAttrColumn
 *        Name of attributes-array column in \em trainingSource 
 * @param classifySource Name of the relation that contains data to be classified
 * @param classifyKeyColumn Name of column in \em classifySource that can
 *        serve as unique identifier (the key of the source relation)
 * @param classifyAttrColumn Name of attributes-array column in \em classifySource
 * @param numAttrs Number of attributes to use for classification
 * @param destName Name of the view to create
 *
 * @internal
 * @sa This function is a wrapper for bayes::create_classification().
 */
CREATE FUNCTION MADLIB_SCHEMA.create_nb_classify_view(
    "trainingSource" VARCHAR,
    "trainingClassColumn" VARCHAR,
    "trainingAttrColumn" VARCHAR,
    "classifySource" VARCHAR,
    "classifyKeyColumn" VARCHAR,
    "classifyAttrColumn" VARCHAR,
    "numAttrs" INTEGER,
    "destName" VARCHAR)
RETURNS VOID
AS $$PythonFunction(bayes, bayes, create_classification_view)$$
LANGUAGE plpythonu VOLATILE;


/**
 * @brief Create view with columns <tt>(key, class, nb_prob)</tt>
 * 
 * The created view will be
 * 
 * <tt>VIEW <em>destName</em> (key, class, nb_prob)</tt>
 * 
 * where \c nb_prob is the Naive-Bayes probability that \c class is the true
 * class of the record in \em classifySource identified by \c key.
 * 
 * @param trainingSource
 *        Name of relation containing the training data
 * @param trainingClassColumn
 *        Name of class column in training data
 * @param trainingAttrColumn
 *        Name of attributes-array column in \em trainingSource 
 * @param classifySource Name of the relation that contains data to be classified
 * @param classifyKeyColumn Name of column in \em classifySource that can
 *        serve as unique identifier (the key of the source relation)
 * @param classifyAttrColumn Name of attributes-array column in \em classifySource
 * @param numAttrs Number of attributes to use for classification
 * @param destName Name of the view to create
 *
 * @internal
 * @sa This function is a wrapper for bayes::create_bayes_probabilities().
 */
CREATE FUNCTION MADLIB_SCHEMA.create_nb_probs_view(
    "trainingSource" VARCHAR,
    "trainingClassColumn" VARCHAR,
    "trainingAttrColumn" VARCHAR,
    "classifySource" VARCHAR,
    "classifyKeyColumn" VARCHAR,
    "classifyAttrColumn" VARCHAR,
    "numAttrs" INTEGER,
    "destName" VARCHAR)
RETURNS VOID
AS $$PythonFunction(bayes, bayes, create_bayes_probabilities_view)$$
LANGUAGE plpythonu VOLATILE;


/**
 * @brief Create view with columns <tt>(key, class, nb_prob)</tt>
 * 
 * The created view will be
 * 
 * <tt>VIEW <em>destName</em> (key, class, nb_prob)</tt>
 * 
 * where \c nb_prob is the Naive-Bayes probability that \c class is the true
 * class of the record in \em classifySource identified by \c key.
 * 
 * @param featureProbsSource Name of table with precomputed feature
 *        probabilities, as created with create_nb_prepared_data_tables()
 * @param classPriorsSource Name of table with precomputed class priors, as
 *        created with create_nb_prepared_data_tables()
 * @param classifySource Name of the relation that contains data to be classified
 * @param classifyKeyColumn Name of column in \em classifySource that can
 *        serve as unique identifier (the key of the source relation)
 * @param classifyAttrColumn Name of attributes-array column in \em classifySource
 * @param numAttrs Number of attributes to use for classification
 * @param destName Name of the view to create
 *
 * @internal
 * @sa This function is a wrapper for bayes::create_bayes_probabilities().
 */
CREATE FUNCTION MADLIB_SCHEMA.create_nb_probs_view(
    "featureProbsSource" VARCHAR,
    "classPriorsSource" VARCHAR,
    "classifySource" VARCHAR,
    "classifyKeyColumn" VARCHAR,
    "classifyAttrColumn" VARCHAR,
    "numAttrs" INTEGER,
    "destName" VARCHAR)
RETURNS VOID
AS $$PythonFunction(bayes, bayes, create_bayes_probabilities_view)$$
LANGUAGE plpythonu VOLATILE;


/**
 * @brief Create a SQL function mapping arrays of attribute values to the Naive
 *        Bayes classification.
 * 
 * The created SQL function will be:
 *  
 * <tt>
 * FUNCTION <em>destName</em> (attributes INTEGER[], smoothingFactor DOUBLE PRECISION)
 * RETURNS INTEGER[]</tt>
 * 
 * @note
 * On Greenplum, the generated SQL function can only be called on the master.
 *
 * @param featureProbsSource Name of table with precomputed feature
 *        probabilities, as created with create_nb_prepared_data_tables()
 * @param classPriorsSource Name of table with precomputed class priors, as
 *        created with create_nb_prepared_data_tables()
 * @param numAttrs Number of attributes to use for classification
 * @param destName Name of the function to create
 *
 * @internal
 * @sa This function is a wrapper for bayes::create_classification_function().
 */
CREATE FUNCTION MADLIB_SCHEMA.create_nb_classify_fn(
    "featureProbsSource" VARCHAR,
    "classPriorsSource" VARCHAR,
    "numAttrs" INTEGER,
    "destName" VARCHAR)
RETURNS VOID
AS $$PythonFunction(bayes, bayes, create_classification_function)$$
LANGUAGE plpythonu VOLATILE;


/**
 * @brief Create a SQL function mapping arrays of attribute values to the Naive
 *        Bayes classification.
 * 
 * The created SQL function will be:
 *  
 * <tt>
 * FUNCTION <em>destName</em> (attributes INTEGER[], smoothingFactor DOUBLE PRECISION)
 * RETURNS INTEGER[]</tt>
 * 
 * @note
 * On Greenplum, the generated SQL function can only be called on the master.
 *
 * @param trainingSource
 *        Name of relation containing the training data
 * @param trainingClassColumn
 *        Name of class column in training data
 * @param trainingAttrColumn
 *        Name of attributes-array column in \em trainingSource 
 * @param numAttrs Number of attributes to use for classification
 * @param destName Name of the function to create
 *
 * @internal
 * @sa This function is a wrapper for bayes::create_classification_function().
 */
CREATE FUNCTION MADLIB_SCHEMA.create_nb_classify_fn(
    "trainingSource" VARCHAR,
    "trainingClassColumn" VARCHAR,
    "trainingAttrColumn" VARCHAR,
    "numAttrs" INTEGER,
    "destName" VARCHAR)
RETURNS VOID
AS $$PythonFunction(bayes, bayes, create_classification_function)$$
LANGUAGE plpythonu VOLATILE;
