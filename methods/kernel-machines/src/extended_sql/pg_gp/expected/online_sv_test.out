-- Example usage for regression:
\i online_sv.sql_in
-- The following is the structure to record the results of a learning process.
-- We work with arrays of float8 for now; we'll extend the code to work with sparse vectors next.
DROP TYPE IF EXISTS madlib.model_rec CASCADE;
psql:online_sv.sql_in:3: NOTICE:  drop cascades to function madlib.online_sv_nd_agg(double precision[])
psql:online_sv.sql_in:3: NOTICE:  drop cascades to function madlib.online_sv_nd_update(madlib.model_rec,double precision[])
psql:online_sv.sql_in:3: NOTICE:  drop cascades to function madlib.online_sv_cl_agg(double precision[],double precision)
psql:online_sv.sql_in:3: NOTICE:  drop cascades to function madlib.online_sv_cl_update(madlib.model_rec,double precision[],double precision)
psql:online_sv.sql_in:3: NOTICE:  drop cascades to function madlib.online_sv_reg_agg(double precision[],double precision)
psql:online_sv.sql_in:3: NOTICE:  drop cascades to function madlib.online_sv_reg_update(madlib.model_rec,double precision[],double precision)
psql:online_sv.sql_in:3: NOTICE:  drop cascades to function madlib.svs_predict(madlib.model_rec,double precision[])
psql:online_sv.sql_in:3: NOTICE:  drop cascades to table madlib.sv_results column model
CREATE TYPE madlib.model_rec AS (
       inds int,        -- number of individuals processed 
       cum_err float8,  -- cumulative error
       epsilon float8,  -- the size of the epsilon tube around the hyperplane, adaptively adjusted by algorithm
       rho float8,      -- classification margin
       b   float8,      -- classifier offset
       nsvs int,        -- number of support vectors
       ind_dim int,     -- the dimension of the individuals
       weights float8[],       -- the weight of the support vectors
       individuals float8[][]  -- the array of support vectors
);
-- Create the necessary tables for storing training data and the learned support vector models
DROP TABLE IF EXISTS madlib.sv_train_data CASCADE;
CREATE TABLE madlib.sv_train_data ( id int, ind float8[], label float8 ) DISTRIBUTED BY (id);
DROP TABLE IF EXISTS madlib.sv_results CASCADE;
CREATE TABLE madlib.sv_results ( id text, model madlib.model_rec ) DISTRIBUTED BY (id);
DROP TABLE IF EXISTS madlib.sv_model CASCADE;
psql:online_sv.sql_in:23: NOTICE:  drop cascades to function madlib.transform_rec(text,integer,double precision[],double precision[])
CREATE TABLE madlib.sv_model ( id text, weight float8, sv float8[] ) DISTRIBUTED BY (weight);
-- Kernel functions are a generalisation of inner products. 
-- They provide the means by which we can extend linear machines to work in non-linear transformed feature spaces.
-- Here we specify the dot product as the kernel; it can be replace with any other kernel, including the polynomial
-- and Gaussian kernels defined below.
CREATE OR REPLACE FUNCTION madlib.kernel(x float8[][], idx int, y float8[]) RETURNS float8 AS $$
DECLARE
	len INT;
	ind float8[];
BEGIN
	len := array_upper(y,1);
	FOR i IN 1..len LOOP
	    ind[i] := x[idx][i];
	END LOOP;
	RETURN madlib.kernel(ind, y);
END
$$ LANGUAGE plpgsql;
CREATE OR REPLACE FUNCTION madlib.kernel(x float8[], y float8[]) RETURNS float8 AS $$
DECLARE
	len INT;
BEGIN
	RETURN madlib.dot_kernel(x, y); -- this doesn't require svecs
--	RETURN dot(ind, y);  -- this does require svecs
--	RETURN madlib.polynomial_kernel(ind, y, 2);
END
$$ LANGUAGE plpgsql;
-- This is just inner product. For efficiency, this can be implemented as a C UDF. In fact, if the sparse vector library
-- is installed, one can just define the body of the function to be RETURN dot(x,y);.
CREATE OR REPLACE FUNCTION madlib.dot_kernel(x float8[], y float8[]) RETURNS float8 AS $$
DECLARE 
	len int;
	ret float8 := 0;
BEGIN
	len := array_upper(y,1);
	FOR i in 1..len LOOP
	    ret := ret + x[i]*y[i];
	END LOOP;
	RETURN ret;
END;
$$ LANGUAGE plpgsql;
-- Here are two other standard kernels.
CREATE OR REPLACE FUNCTION madlib.polynomial_kernel(x float8[], y float8[], degree int) RETURNS float8 AS $$
BEGIN
	RETURN madlib.dot_kernel(x,y) ^ degree;
END;
$$ LANGUAGE plpgsql;
CREATE OR REPLACE FUNCTION madlib.gaussian_kernel(x float8[], y float8[], gamma float) RETURNS float8 AS $$
BEGIN
	RETURN exp(-1.0 * gamma * madlib.dot_kernel(x-y,x-y));
END;
$$ LANGUAGE plpgsql;
-- This implements the prediction function: f(x) = sum_i weight[i] * madlib.kernel(suppor_vector[i], x).
CREATE OR REPLACE FUNCTION
madlib.svs_predict(svs madlib.model_rec, ind float8[]) 
RETURNS float8 AS $$
DECLARE
	ret FLOAT8 := 0;
BEGIN
	FOR i IN 1..svs.nsvs LOOP
	    ret := ret + svs.weights[i] * madlib.kernel(svs.individuals, i, ind);
        END LOOP;
	RETURN ret;
END;
$$ LANGUAGE plpgsql;
-- This is the main online support vector regression learning algorithm. 
-- The function updates the support vector model as it processes each new training example.
-- This function is wrapped in an aggregate function to process all the training examples stored in a table.  
-- The learning parameters (eta, slambda, and nu) are hardcoded at the moment. 
-- We may want to make them input parameters at some stage, although the naive user would probably be daunted with the prospect
-- of having to specify them. 
CREATE OR REPLACE FUNCTION madlib.online_sv_reg_update(svs madlib.model_rec, ind float8[], label float8) 
RETURNS madlib.model_rec AS $$
DECLARE
	eta FLOAT8 := 0.05; -- learning rate
	slambda FLOAT8 := 0.2;  -- regularisation parameter
	nu FLOAT8 := 0.001;     -- compression parameter, a number between 0 and 1; the fraction of the training data that appear as support vectors
	p FLOAT8;       -- prediction for the input individual
	diff FLOAT8;    -- difference between p and label
	error FLOAT8;   -- absolute value of diff
	weight FLOAT8;  -- the weight of ind if it turns out to be a support vector
BEGIN
	IF svs IS NULL THEN
	    svs := (0, 0, 0, 0.5, 1, 1, array_upper(ind,1), '{0}', array[ind]);  -- we have to be careful to initialise a multi-dimensional array
        END IF;

	p := madlib.svs_predict(svs, ind);
	diff := label - p;
	error := abs(diff);
	svs.inds := svs.inds + 1;
	svs.cum_err := svs.cum_err + error;

	IF (error > svs.epsilon) THEN
	    FOR i IN 1..svs.nsvs LOOP -- Unlike the original algorithm, this rescaling is only done when we make a large enough error.
	    	 svs.weights[i] := svs.weights[i] * (1 - eta * slambda);
            END LOOP;

	    weight := eta;
	    IF (diff < 0) THEN weight := -1 * weight; END IF;
	    svs.nsvs := svs.nsvs + 1;
	    svs.weights[svs.nsvs] := weight;
	    svs.individuals := array_cat(svs.individuals, ind);
	    svs.epsilon := svs.epsilon + (1 - nu) * eta;      
	ELSE
	    svs.epsilon := svs.epsilon - eta * nu;
        END IF;

	return svs;
END
$$ LANGUAGE plpgsql;
DROP AGGREGATE IF EXISTS madlib.online_sv_reg_agg(float8[], float8);
psql:online_sv.sql_in:143: NOTICE:  aggregate madlib.online_sv_reg_agg(float8[],float8) does not exist, skipping
CREATE AGGREGATE madlib.online_sv_reg_agg(float8[], float8) (
       sfunc = madlib.online_sv_reg_update,
       stype = madlib.model_rec
);
-- This is the main online support vector classification algorithm. 
-- The function updates the support vector model as it processes each new training example.
-- This function is wrapped in an aggregate function to process all the training examples stored in a table.  
-- The learning parameters (eta and nu) are hardcoded at the moment. 
-- We may want to make them input parameters at some stage, although the naive user would probably be daunted with the prospect
-- of having to specify them. 
CREATE OR REPLACE FUNCTION madlib.online_sv_cl_update(svs madlib.model_rec, ind float8[], label float8) 
RETURNS madlib.model_rec AS $$
DECLARE
	eta FLOAT8 := 0.05; -- learning rate
	nu FLOAT8 := 0.2;     -- the fraction of the training data with margin error, a number between 0 and 1; small nu => large margin and more support vectors
	p FLOAT8;       -- prediction for the input individual
BEGIN
	IF svs IS NULL THEN
	    svs := (0, 0, 0, 0.5, 1, 1, array_upper(ind,1), '{0}', array[ind]);  -- we have to be careful to initialise a multi-dimensional array
        END IF;

	p := label * (madlib.svs_predict(svs, ind) + svs.b);
	svs.inds := svs.inds + 1;
	IF p < 0 THEN
	    svs.cum_err := svs.cum_err + 1;
        END IF;

	IF (p < svs.rho) THEN
	    FOR i IN 1..svs.nsvs LOOP -- Unlike the original algorithm, this rescaling is only done when we make a margin error.
	    	 svs.weights[i] := svs.weights[i] * (1 - eta);
            END LOOP;

	    svs.nsvs := svs.nsvs + 1;
	    svs.weights[svs.nsvs] := label * eta;
	    svs.individuals := array_cat(svs.individuals, ind);
	    svs.b := svs.b + eta * label;
	    svs.rho := svs.rho + eta * (1 - nu);
	ELSE
	    svs.rho := svs.rho - eta * nu;
        END IF;

	return svs;
END
$$ LANGUAGE plpgsql;
-- This is the main online support vector novelty detection algorithm. 
-- The function updates the support vector model as it processes each new training example.
-- In contrast to classification and regression, the training data points have no labels.
-- This function is wrapped in an aggregate function to process all the training examples stored in a table.  
-- The learning parameters (eta and nu) are hardcoded at the moment. 
-- We may want to make them input parameters at some stage, although the naive user would probably be daunted with the prospect
-- of having to specify them. 
DROP AGGREGATE IF EXISTS madlib.online_sv_cl_agg(float8[], float8);
psql:online_sv.sql_in:197: NOTICE:  aggregate madlib.online_sv_cl_agg(float8[],float8) does not exist, skipping
CREATE AGGREGATE madlib.online_sv_cl_agg(float8[], float8) (
       sfunc = madlib.online_sv_cl_update,
       stype = madlib.model_rec
);
CREATE OR REPLACE FUNCTION madlib.online_sv_nd_update(svs madlib.model_rec, ind float8[]) 
RETURNS madlib.model_rec AS $$
DECLARE
	eta FLOAT8 := 0.1; -- learning rate
	nu FLOAT8 := 0.05;  -- the fraction of the training data with margin error, a number between 0 and 1
	p FLOAT8;       -- prediction for the input individual
BEGIN
	IF svs IS NULL THEN
	    svs := (0, 0, 0, 0.5, 1, 1, array_upper(ind,1), '{0}', array[ind]);  -- we have to be careful to initialise a multi-dimensional array
        END IF;

	p := madlib.svs_predict(svs, ind);
	svs.inds := svs.inds + 1;

	IF (p < svs.rho) THEN
	    FOR i IN 1..svs.nsvs LOOP -- Unlike the original algorithm, this rescaling is only done when we make a margin error.
	    	 svs.weights[i] := svs.weights[i] * (1 - eta);
            END LOOP;

	    svs.nsvs := svs.nsvs + 1;
	    svs.weights[svs.nsvs] := eta;
	    svs.individuals := array_cat(svs.individuals, ind);
	    svs.rho := svs.rho + eta * (1 - nu);
	ELSE
	    svs.rho := svs.rho - eta * nu;
        END IF;

	return svs;
END
$$ LANGUAGE plpgsql;
DROP AGGREGATE IF EXISTS madlib.online_sv_nd_agg(float8[]);
psql:online_sv.sql_in:235: NOTICE:  aggregate madlib.online_sv_nd_agg(float8[]) does not exist, skipping
CREATE AGGREGATE madlib.online_sv_nd_agg(float8[]) (
       sfunc = madlib.online_sv_nd_update,
       stype = madlib.model_rec
);
-- This function transforms a madlib.model_rec into a set of (weight, support_vector) values for the purpose of storage in a table.
CREATE OR REPLACE FUNCTION madlib.transform_rec(modelname text, ind_dim int, weights float8[], individuals float8[][]) RETURNS SETOF madlib.sv_model AS $$
DECLARE
	nsvs INT;
	sv madlib.sv_model;
BEGIN
	nsvs = array_upper(weights,1);
	FOR i IN 1..nsvs LOOP 
	    sv.id = modelname;
       	    sv.weight = weights[i];
	    FOR j IN 1..ind_dim LOOP sv.sv[j] = individuals[i][j]; END LOOP; -- we copy the individual because we can't say sv.sv[j] = individuals[i]
	    RETURN NEXT sv;
     	END LOOP;
END;
$$ LANGUAGE plpgsql;
-- This function stores a madlib.model_rec stored with modelname in the madlib.sv_results table into the madlib.sv_model table.
CREATE OR REPLACE FUNCTION madlib.storeModel(modelname TEXT) RETURNS VOID AS $$
DECLARE
	myind_dim INT;
	myweights float8[];
	myindividuals float8[][];
--	mysvs madlib.model_rec;
BEGIN
--	SELECT INTO mysvs model FROM madlib.sv_results WHERE id = modelname; -- for some strange reason this line doesn't work....
	SELECT INTO myind_dim (model).ind_dim FROM madlib.sv_results WHERE id = modelname;
	SELECT INTO myweights (model).weights FROM madlib.sv_results WHERE id = modelname;
	SELECT INTO myindividuals (model).individuals FROM madlib.sv_results WHERE id = modelname;
 	INSERT INTO madlib.sv_model (SELECT * FROM madlib.transform_rec(modelname, myind_dim, myweights, myindividuals));
END;
$$ LANGUAGE plpgsql;
-- This function stores a collection of models learned in parallel into the madlib.sv_model table.
-- The different models are assumed to be named modelname1, modelname2, ....
CREATE OR REPLACE FUNCTION madlib.storeModel(modelname TEXT, n INT) RETURNS VOID AS $$
DECLARE
BEGIN
	FOR i IN 0..n-1 LOOP
	    PERFORM madlib.storeModel(modelname || i);
        END LOOP;
END;
$$ LANGUAGE plpgsql;
-- This function performs prediction using a support vector machine stored in the madlib.sv_model table.
CREATE OR REPLACE FUNCTION madlib.svs_predict(modelname text, ind float8[], OUT ret float8) RETURNS FLOAT8 AS $$
BEGIN
	SELECT INTO ret sum(weight * madlib.kernel(sv, ind)) FROM madlib.sv_model WHERE id = modelname;
END;
$$ LANGUAGE plpgsql;
DROP TYPE IF EXISTS madlib.model_pr CASCADE;
psql:online_sv.sql_in:292: NOTICE:  drop cascades to function madlib.svs_predict_combo(text,double precision[])
CREATE TYPE madlib.model_pr AS ( model text, prediction float8 );
-- This function performs prediction using the support vector machines stored in the madlib.sv_model table.
-- The different models are assumed to be named modelname1, modelname2, ....
-- An average prediction is given at the end.
CREATE OR REPLACE FUNCTION madlib.svs_predict_combo(modelname text, ind float8[]) RETURNS SETOF madlib.model_pr AS $$
DECLARE
	sumpr float8 := 0;
	mpr madlib.model_pr;
	n int;
BEGIN
	SELECT INTO n COUNT(DISTINCT(id)) FROM madlib.sv_model WHERE position(modelname in id) > 0 AND modelname <> id;
	FOR i IN 0..n-1 LOOP
	    mpr.model := modelname || i;
	    mpr.prediction := madlib.svs_predict(mpr.model, ind);
	    sumpr := sumpr + mpr.prediction;
	    RETURN NEXT mpr;
 	END LOOP;
	mpr.model := 'avg';
	mpr.prediction := sumpr / n;
	RETURN NEXT mpr;
END;
$$ LANGUAGE plpgsql;
-- This function removes all the models whose id is prefixed with modelname in the sv_results table
CREATE OR REPLACE FUNCTION
madlib.drop_sv_model(modelname text)
RETURNS VOID
AS $$
   sql = 'DELETE FROM madlib.sv_results WHERE position(\'' + modelname + '\' in id) > 0';
   plpy.execute(sql);
$$ LANGUAGE 'plpythonu';
/* 
 This is the support vector regression function.
 Parameters:
   input_table: the name of the table/view with the training data;
   model_name : the name under which we want to store the resultant learned model; 
   parallel   : a flag indicating whether the system should learn multiple models in parallel.
*/
CREATE OR REPLACE FUNCTION 
madlib.sv_regression(tablename text, modelname text, parallel bool)
RETURNS VOID
AS $$
   if (parallel) :
      sql = 'select count(*) from madlib.sv_results where id = \'' + modelname + '0\'';
      seen = plpy.execute(sql);
      if (seen[0]['count'] > 0):
      	  plpy.error('model with name \'' + modelname + '\' already exists; please use a different model name or drop the model using drop_sv_model() function');

      sql = 'insert into madlib.sv_results (select \'' + modelname + '\' || gp_segment_id, madlib.online_sv_reg_agg(ind, label) from ' + tablename + ' group by gp_segment_id)';
      plpy.execute(sql);
      numproc_t = plpy.execute('select count(distinct(gp_segment_id)) from ' + tablename);
      numproc = numproc_t[0]['count'];
      plpy.execute('select madlib.storeModel(\'' + modelname + '\', ' + str(numproc) + ')');     
   else :
      sql = 'select count(*) from madlib.sv_results where id = \'' + modelname + '\'';
      seen = plpy.execute(sql);
      if (seen[0]['count'] > 0):
      	  plpy.error('model with name \'' + modelname + '\' already exists; please use a different model name or drop the model using drop_sv_model() function');
      sql = 'insert into madlib.sv_results (select \'' + modelname + '\', madlib.online_sv_reg_agg(ind, label) from ' + tablename + ')';
      plpy.execute(sql);
      plpy.execute('select madlib.storeModel(\'' + modelname + '\')');
$$ LANGUAGE 'plpythonu';
/* 
 This is the support vector classification function.
 Parameters:
   input_table: the name of the table/view with the training data;
   model_name : the name under which we want to store the resultant learned model; 
   parallel   : a flag indicating whether the system should learn multiple models in parallel.
*/
CREATE OR REPLACE FUNCTION 
madlib.sv_classification(tablename text, modelname text, parallel bool)
RETURNS VOID
AS $$
   if (parallel) :
      sql = 'select count(*) from madlib.sv_results where id = \'' + modelname + '0\'';
      seen = plpy.execute(sql);
      if (seen[0]['count'] > 0):
      	  plpy.error('model with name \'' + modelname + '\' already exists; please use a different model name or drop the model using drop_sv_model() function');

      sql = 'insert into madlib.sv_results (select \'' + modelname + '\' || gp_segment_id, madlib.online_sv_cl_agg(ind, label) from ' + tablename + ' group by gp_segment_id)';
      plpy.execute(sql);
      numproc_t = plpy.execute('select count(distinct(gp_segment_id)) from ' + tablename);
      numproc = numproc_t[0]['count'];
      plpy.execute('select madlib.storeModel(\'' + modelname + '\', ' + str(numproc) + ')');     
   else :
      sql = 'select count(*) from madlib.sv_results where id = \'' + modelname + '\'';
      seen = plpy.execute(sql);
      if (seen[0]['count'] > 0):
      	  plpy.error('model with name \'' + modelname + '\' already exists; please use a different model name or drop the model using drop_sv_model() function');
      sql = 'insert into madlib.sv_results (select \'' + modelname + '\', madlib.online_sv_cl_agg(ind, label) from ' + tablename + ')';
      plpy.execute(sql);
      plpy.execute('select madlib.storeModel(\'' + modelname + '\')');
$$ LANGUAGE 'plpythonu';
/* 
 This is the support vector classification function.
 Parameters:
   input_table: the name of the table/view with the training data;
   model_name : the name under which we want to store the resultant learned model; 
   parallel   : a flag indicating whether the system should learn multiple models in parallel.
*/
CREATE OR REPLACE FUNCTION 
madlib.sv_novelty_detection(tablename text, modelname text, parallel bool)
RETURNS VOID
AS $$
   if (parallel) :
      sql = 'select count(*) from madlib.sv_results where id = \'' + modelname + '0\'';
      seen = plpy.execute(sql);
      if (seen[0]['count'] > 0):
      	  plpy.error('model with name \'' + modelname + '\' already exists; please use a different model name or drop the model using drop_sv_model() function');

      sql = 'insert into madlib.sv_results (select \'' + modelname + '\' || gp_segment_id, madlib.online_sv_nd_agg(ind) from ' + tablename + ' group by gp_segment_id)';
      plpy.execute(sql);
      numproc_t = plpy.execute('select count(distinct(gp_segment_id)) from ' + tablename);
      numproc = numproc_t[0]['count'];
      plpy.execute('select madlib.storeModel(\'' + modelname + '\', ' + str(numproc) + ')');     
   else :
      sql = 'select count(*) from madlib.sv_results where id = \'' + modelname + '\'';
      seen = plpy.execute(sql);
      if (seen[0]['count'] > 0):
      	  plpy.error('model with name \'' + modelname + '\' already exists; please use a different model name or drop the model using drop_sv_model() function');
      sql = 'insert into madlib.sv_results (select \'' + modelname + '\', madlib.online_sv_nd_agg(ind) from ' + tablename + ')';
      plpy.execute(sql);
      plpy.execute('select madlib.storeModel(\'' + modelname + '\')');
$$ LANGUAGE 'plpythonu';
-- Generate artificial training data 
CREATE OR REPLACE FUNCTION madlib.randomInd(d INT) RETURNS float8[] AS $$
DECLARE
    ret float8[];
BEGIN
    FOR i IN 1..(d-1) LOOP
        ret[i] = RANDOM() * 40 - 20;
    END LOOP;
    IF (RANDOM() > 0.5) THEN
        ret[d] = 10;
    ELSE 
        ret[d] = -10;
    END IF;
    RETURN ret;
END
$$ LANGUAGE plpgsql;
CREATE OR REPLACE FUNCTION madlib.randomInd2(d INT) RETURNS float8[] AS $$
DECLARE
    ret float8[];
BEGIN
    FOR i IN 1..d LOOP
        ret[i] = RANDOM() * 5 + 10;
    END LOOP;
    RETURN ret;
END
$$ LANGUAGE plpgsql;
CREATE OR REPLACE FUNCTION madlib.generateRegData(num int, dim int) RETURNS VOID AS $$
    plpy.execute("DELETE FROM madlib.sv_train_data")
    plpy.execute("INSERT INTO madlib.sv_train_data SELECT a.val, madlib.randomInd(" + str(dim) + "), 0 FROM (SELECT generate_series(1," + str(num) + ") AS val) AS a")
    plpy.execute("UPDATE madlib.sv_train_data SET label = madlib.targetRegFunc(ind)")
$$ LANGUAGE 'plpythonu';
CREATE OR REPLACE FUNCTION madlib.targetRegFunc(ind float8[]) RETURNS float8 AS $$
DECLARE
    dim int;
BEGIN
    dim = array_upper(ind,1);
    IF (ind[dim] = 10) THEN RETURN 50; END IF;
    RETURN -50;
END
$$ LANGUAGE plpgsql;
CREATE OR REPLACE FUNCTION madlib.generateClData(num int, dim int) RETURNS VOID AS $$
    plpy.execute("DELETE FROM madlib.sv_train_data")
    plpy.execute("INSERT INTO madlib.sv_train_data SELECT a.val, madlib.randomInd(" + str(dim) + "), 0 FROM (SELECT generate_series(1," + str(num) + ") AS val) AS a")
    plpy.execute("UPDATE madlib.sv_train_data SET label = madlib.targetClFunc(ind)")
$$ LANGUAGE 'plpythonu';
CREATE OR REPLACE FUNCTION madlib.targetClFunc(ind float8[]) RETURNS float8 AS $$
BEGIN
    IF (ind[1] > 0 AND ind[2] < 0) THEN RETURN 1; END IF;
    RETURN -1;
END
$$ LANGUAGE plpgsql;
CREATE OR REPLACE FUNCTION madlib.generateNdData(num int, dim int) RETURNS VOID AS $$
    plpy.execute("DELETE FROM madlib.sv_train_data")
    plpy.execute("INSERT INTO madlib.sv_train_data SELECT a.val, madlib.randomInd2(" + str(dim) + "), 0 FROM (SELECT generate_series(1," + str(num) + ") AS val) AS a")
$$ LANGUAGE 'plpythonu';
select madlib.generateRegData(1000, 5);
 generateregdata 
-----------------
 
(1 row)

select madlib.sv_regression('madlib.sv_train_data', 'myexp', false);
 sv_regression 
---------------
 
(1 row)

select madlib.svs_predict('myexp', '{1,2,4,20,10}') > 0;
 ?column? 
----------
 t
(1 row)

select madlib.svs_predict('myexp', '{1,2,4,20,-10}') < 0;
 ?column? 
----------
 t
(1 row)

   
-- To learn multiple support vector regression models
select madlib.sv_regression('madlib.sv_train_data', 'myexp', true);
 sv_regression 
---------------
 
(1 row)

select pred.prediction > 0 from madlib.svs_predict_combo('myexp', '{1,2,4,20,10}') as pred;
 ?column? 
----------
 t
 t
 t
(3 rows)

-- Example usage for classification:
select madlib.generateClData(1000, 2);
 generatecldata 
----------------
 
(1 row)

select madlib.sv_classification('madlib.sv_train_data', 'myexpc', false);
 sv_classification 
-------------------
 
(1 row)

select madlib.svs_predict('myexpc', '{10,-20}') > 0;
 ?column? 
----------
 t
(1 row)

select madlib.svs_predict('myexpc', '{-10,20}') < 0;
 ?column? 
----------
 t
(1 row)

-- To learn multiple support vector models, replace the above by 
select madlib.sv_classification('madlib.sv_train_data', 'myexpc', true);
 sv_classification 
-------------------
 
(1 row)

select pred.prediction > 0 from madlib.svs_predict_combo('myexpc', '{10,-20}') as pred;
 ?column? 
----------
 t
 t
 t
(3 rows)

select pred.prediction < 0 from madlib.svs_predict_combo('myexpc', '{-10,20}') as pred;
 ?column? 
----------
 t
 t
 t
(3 rows)

-- Example usage for novelty detection:
select madlib.generateNdData(500, 2);
 generatenddata 
----------------
 
(1 row)

select madlib.sv_novelty_detection('madlib.sv_train_data', 'myexpnd', false);
 sv_novelty_detection 
----------------------
 
(1 row)

select madlib.svs_predict('myexpnd', '{10,-5}') > 0;  
 ?column? 
----------
 t
(1 row)

select madlib.svs_predict('myexpnd', '{-1,-1}') < 0;  
 ?column? 
----------
 t
(1 row)

