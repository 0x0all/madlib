#!/usr/bin/env python

import plpy

"""@namespace kernmach

@file online_sv.py_in

@addtogroup grp_kernmach

@about

    Support vector machines (SVMs) and related kernel methods have been one of 
    the most popular and well-studied machine learning techniques of the 
    past 15 years, with an amazing number of innovations and applications.

    In a nutshell, an SVM model f(x) takes the form of
\code
        f(x) = sum_i alpha_i k(x_i,x),
\endcode
    where each alpha_i is a real number, each x_i is a data point from the
    training set (called a support vector), and k(.,.) is a kernel function 
    that measures how `similar' two objects are. In regression, f(x) is the 
    regression function we seek. In classification, f(x) serves as the 
    decision boundary; so for example in binary classification, the predictor 
    can output class 1 for object x if f(x) >= 0, and class 2 otherwise.

    In the case when the kernel function k(.,.) is the standard inner product 
    on vectors, f(x) is just an alternative way of writing a linear function
\code
        f'(x) = <w,x>,
\endcode
    where w is a weight vector having the same dimension as x. One of the 
    key points of SVMs is that we can use more fancy kernel functions to 
    efficiently learn linear models in high-dimensional feature spaces, 
    since k(x_i,x_j) can be understood as an efficient way of computing an 
    inner product in the feature space:
\code
        k(x_i,x_j) = <phi(x_i),phi(x_j)>,
\endcode
    where phi(x) projects x into a (possibly infinite-dimensional) feature
    space.

    There are many algorithms for learning kernel machines. This module 
    implements the class of online learning with kernels algorithms 
    described in \n

      - Jyrki Kivinen, Alexander J. Smola and Robert C. Williamson, \n
        Online Learning with Kernels, IEEE Transactions on Signal Processing, 52(8), 2165-2176, 2004.\n

    See also the book \n
  
      - Bernhard Scholkopf and Alexander J. Smola, Learning with Kernels: \n
        Support Vector Machines, Regularization, Optimization, and Beyond, 
        MIT Press, 2002 \n
 
    for much more details.

    The implementation follows the original description in the Kivinen et al 
    paper faithfully, except that we only update the support vector model 
    when we make a significant error. The original algorithms update the 
    support vector model at every step, even when no error was made, in the 
    name of regularisation. For practical purposes, and this is verified 
    empirically to a certain degree, updating only when necessary is both 
    faster and better from a learning-theoretic point of view, at least in 
    the i.i.d. setting.

    Methods for classification, regression and novelty detection are 
    available. Multiple instances of the algorithms can be executed 
    in parallel on different subsets of the training data. The resultant
    support vector models can then be combined using standard techniques
    like averaging or majority voting.

    Training data points are accessed via a table or a view. The support
    vector models can also be stored in tables for fast execution.


@prereq

    - None at this point. Will need the Greenplum sparse vector 
      SVEC datatype eventually.

@usage

  Here are the main learning functions.

     -  Regression learning is achieved through the following function
        \code
        madlib.sv_regression(input_table text, model_name text, parallel bool)
        \endcode

     -  Classification learning is achieved through the following function
        \code
        madlib.sv_classification(input_table text, model_name text, parallel bool)
        \endcode

     -  Novelty detection is achieved through the following function
        \code
        madlib.sv_novelty_detection(input_table text, model_name text, parallel bool)
        \endcode

  In each case, input_table is the name of the table/view with the training 
  data, model_name is the name under which we want to store the resultant 
  learned model, and parallel is a flag indicating whether the system
  should learn multiple models in parallel. (The multiple models can be
  combined to make predictions; more on that shortly.)

  Here are the functions that can be used to make predictions on new
  data points.

     - To make predictions on new data points using a single model
       learned previously, we use the function
       \code
       madlib.svs_predict(model_name text, x float8[]),
       \endcode
       where model_name is the name of the model stored and x is a data point.

     - To make predictions on new data points using multiple models
       learned in parallel, we use the function
       \code
       madlib.svs_predict_combo(model_name text, x float8[]),
       \endcode
       where model_name is the name under which the models are stored, and x 
       is a data point.

     - To make predictions on new data points stored in a table using
       previously learned models, we use the function
       \code
       madlib.sv_predict(input_table text, col_name text, model_name text, output_table text, parallel bool),
       \endcode
       where the data points are stored under column col_name in input_table,
       output_table is the table into which we will store the results,
       model_name is the name of the model previously learned, and parallel is 
       true if the model was learned in parallel, false otherwise. 
       The output_table is created during the function call; an existing table with 
       the same name will be dropped.

  Models that have been stored can be deleted using the function
  \code
       madlib.drop_sv_model(modelname text).
  \endcode

@examp

As a general first step, we need to prepare and populate an input 
table/view with the following structure:
\code   
        CREATE TABLE my_schema.my_input_table 
        (       
                id    INT,       -- point ID
                ind   FLOAT8[],  -- data point
                label FLOAT8     -- label of data point
    	);
\endcode    
     Note: The label field is not required for novelty detection.
    

Example usage for regression:
     -# We can randomly generate 1000 5-dimensional data labelled by the simple target function 
        \code
        t(x) = if x[5] = 10 then 50 else if x[5] = -10 then 50 else 0;
        \endcode
        and store that in the madlib.sv_train_data table as follows:
        \code
        testdb=# select madlib.generateRegData(1000, 5);
        \endcode
     -# We can now learn a regression model and store the resultant model
        under the name  'myexp'.
        \code
        testdb=# select madlib.sv_regression('madlib.sv_train_data', 'myexp', false);
        \endcode
     -# We can now start using it to predict the labels of new data points 
        like as follows:
        \code
        testdb=# select madlib.svs_predict('myexp', '{1,2,4,20,10}');
        testdb=# select madlib.svs_predict('myexp', '{1,2,4,20,-10}');
        \endcode
     -# To learn multiple support vector models, we replace the learning step above by 
        \code
        testdb=# select madlib.sv_regression('madlib.sv_train_data', 'myexp', true);
        \endcode
        The resultant models can be used for prediction as follows:
        \code
        testdb=# select * from madlib.svs_predict_combo('myexp', '{1,2,4,20,10}');
        \endcode
     -# We can also predict the labels of all the data points stored in a table.
        For example, we can execute the following:
        \code
        testdb=# create table madlib.sv_reg_test ( id int, ind float8[] );
        testdb=# insert into madlib.sv_reg_test (select id, ind from madlib.sv_train_data limit 20);
        testdb=# select madlib.sv_predict('madlib.sv_reg_test', 'ind', 'myexp', 'madlib.sv_reg_output1', false); 
        testdb=# select * from madlib.sv_reg_output1;
        testdb=# select madlib.sv_predict('madlib.sv_reg_test', 'ind', 'myexp', 'madlib.sv_reg_output2', true);
        testdb=# select * from madlib.sv_reg_output2;
        \endcode 

Example usage for classification:
     -# We can randomly generate 2000 5-dimensional data labelled by the simple
        target function 
        \code
        t(x) = if x[1] > 0 and  x[2] < 0 then 1 else -1;
        \endcode
        and store that in the madlib.sv_train_data table as follows:
        \code 
        testdb=# select madlib.generateClData(2000, 5);
        \endcode
     -# We can now learn a classification model and store the resultant model
        under the name  'myexpc'.
        \code
        testdb=# select madlib.sv_classification('madlib.sv_train_data', 'myexpc', false);
        \endcode
     -# We can now start using it to predict the labels of new data points 
        like as follows:
        \code
        testdb=# select madlib.svs_predict('myexpc', '{10,-2,4,20,10}');
        \endcode 
     -# To learn multiple support vector models, replace the model-building and prediction steps above by 
        \code
        testdb=# select madlib.sv_classification('madlib.sv_train_data', 'myexpc', true);
        testdb=# select * from madlib.svs_predict_combo('myexpc', '{10,-2,4,20,10}');
        \endcode

Example usage for novelty detection:
     -# We can randomly generate 100 2-dimensional data (the normal cases)
        and store that in the madlib.sv_train_data table as follows:
        \code
        testdb=# select madlib.generateNdData(100, 2);
        \endcode
     -# Learning and predicting using a single novelty detection model can be done as follows:
        \code
        testdb=# select madlib.sv_novelty_detection('madlib.sv_train_data', 'myexpnd', false);
        testdb=# select madlib.svs_predict('myexpnd', '{10,-10}');  
        testdb=# select madlib.svs_predict('myexpnd', '{-1,-1}');  
        \endcode
     -# Learning and predicting using multiple models can be done as follows:
        \code
        testdb=# select madlib.sv_novelty_detection('madlib.sv_train_data', 'myexpnd', true);
        testdb=# select * from madlib.svs_predict_combo('myexpnd', '{10,-10}');  
        testdb=# select * from madlib.svs_predict_combo('myexpnd', '{-1,-1}');  
        \endcode

"""

# -----------------------------------------------
# Function to run the regression algorithm
# -----------------------------------------------
def sv_regression( input_table, modelname, parallel):
    """
    Executes the support vector regression algorithm.

    @param input_table Name of table/view containing the training data
    @param model_name Name under which we want to store the learned model 
    @param parallel A flag indicating whether the system should learn multiple models in parallel.
    
    """

    if (parallel) :
        # Learning multiple models in parallel  

        # Output error if models with the same modelname already exist
        sql = 'select count(*) from MADLIB_SCHEMA.sv_results where id = \'' + modelname + '0\'';
        seen = plpy.execute(sql);
        if (seen[0]['count'] > 0):
            plpy.error('model with name \'' + modelname + '\' already exists; please use a different model name or drop the model using drop_sv_model() function');

        # Start learning process
        sql = 'insert into MADLIB_SCHEMA.sv_results (select \'' + modelname + '\' || gp_segment_id, MADLIB_SCHEMA.online_sv_reg_agg(ind, label) from ' + input_table + ' group by gp_segment_id)';
        plpy.execute(sql);

        # Store the models learned
        numproc_t = plpy.execute('select count(distinct(gp_segment_id)) from ' + input_table);
        numproc = numproc_t[0]['count'];
        plpy.execute('select MADLIB_SCHEMA.storeModel(\'' + modelname + '\', ' + str(numproc) + ')');     
    else :
        # Learning a single model

        # Output error if a model with the same modelname already exists
        sql = 'select count(*) from MADLIB_SCHEMA.sv_results where id = \'' + modelname + '\'';
        seen = plpy.execute(sql);
        if (seen[0]['count'] > 0):
            plpy.error('model with name \'' + modelname + '\' already exists; please use a different model name or drop the model using drop_sv_model() function');
        
        # Start learning process    
        sql = 'insert into MADLIB_SCHEMA.sv_results (select \'' + modelname + '\', MADLIB_SCHEMA.online_sv_reg_agg(ind, label) from ' + input_table + ')';
        plpy.execute(sql);

        # Store the model learned
        plpy.execute('select MADLIB_SCHEMA.storeModel(\'' + modelname + '\')');

    return '''Finished support vector regression learning on %s table. 
           ''' % (input_table)

# -----------------------------------------------
# Function to run the classification algorithm
# -----------------------------------------------
def sv_classification( input_table, modelname, parallel):
    """
    Executes the support vector classification algorithm.

    @param input_table Name of table/view containing the training data
    @param model_name Name under which we want to store the learned model 
    @param parallel A flag indicating whether the system should learn multiple models in parallel.
    
    """

    if (parallel) :
        # Learning multiple models in parallel  

        # Output error if models with the same modelname already exist
        sql = 'select count(*) from MADLIB_SCHEMA.sv_results where id = \'' + modelname + '0\'';
        seen = plpy.execute(sql);
        if (seen[0]['count'] > 0):
            plpy.error('model with name \'' + modelname + '\' already exists; please use a different model name or drop the model using drop_sv_model() function');

        # Start learning process
        sql = 'insert into MADLIB_SCHEMA.sv_results (select \'' + modelname + '\' || gp_segment_id, MADLIB_SCHEMA.online_sv_cl_agg(ind, label) from ' + input_table + ' group by gp_segment_id)';
        plpy.execute(sql);

        # Store the models learned
        numproc_t = plpy.execute('select count(distinct(gp_segment_id)) from ' + input_table);
        numproc = numproc_t[0]['count'];
        plpy.execute('select MADLIB_SCHEMA.storeModel(\'' + modelname + '\', ' + str(numproc) + ')');     
    else :
        # Learning a single model

        # Output error if a model with the same modelname already exists
        sql = 'select count(*) from MADLIB_SCHEMA.sv_results where id = \'' + modelname + '\'';
        seen = plpy.execute(sql);
        if (seen[0]['count'] > 0):
            plpy.error('model with name \'' + modelname + '\' already exists; please use a different model name or drop the model using drop_sv_model() function');
        
        # Start learning process    
        sql = 'insert into MADLIB_SCHEMA.sv_results (select \'' + modelname + '\', MADLIB_SCHEMA.online_sv_cl_agg(ind, label) from ' + input_table + ')';
        plpy.execute(sql);

        # Store the model learned
        plpy.execute('select MADLIB_SCHEMA.storeModel(\'' + modelname + '\')');

    return '''Finished support vector classification learning on %s table. 
           ''' % (input_table)


# -----------------------------------------------
# Function to run the novelty detection algorithm
# -----------------------------------------------
def sv_novelty_detection( input_table, modelname, parallel):
    """
    Executes the support vector novelty detection algorithm.

    @param input_table Name of table/view containing the training data
    @param model_name Name under which we want to store the learned model 
    @param parallel A flag indicating whether the system should learn multiple models in parallel.
    
    """

    if (parallel) :
        # Learning multiple models in parallel  

        # Output error if models with the same modelname already exist
        sql = 'select count(*) from MADLIB_SCHEMA.sv_results where id = \'' + modelname + '0\'';
        seen = plpy.execute(sql);
        if (seen[0]['count'] > 0):
            plpy.error('model with name \'' + modelname + '\' already exists; please use a different model name or drop the model using drop_sv_model() function');

        # Start learning process
        sql = 'insert into MADLIB_SCHEMA.sv_results (select \'' + modelname + '\' || gp_segment_id, MADLIB_SCHEMA.online_sv_nd_agg(ind) from ' + input_table + ' group by gp_segment_id)';
        plpy.execute(sql);

        # Store the models learned
        numproc_t = plpy.execute('select count(distinct(gp_segment_id)) from ' + input_table);
        numproc = numproc_t[0]['count'];
        plpy.execute('select MADLIB_SCHEMA.storeModel(\'' + modelname + '\', ' + str(numproc) + ')');     
    else :
        # Learning a single model

        # Output error if a model with the same modelname already exists
        sql = 'select count(*) from MADLIB_SCHEMA.sv_results where id = \'' + modelname + '\'';
        seen = plpy.execute(sql);
        if (seen[0]['count'] > 0):
            plpy.error('model with name \'' + modelname + '\' already exists; please use a different model name or drop the model using drop_sv_model() function');
        
        # Start learning process    
        sql = 'insert into MADLIB_SCHEMA.sv_results (select \'' + modelname + '\', MADLIB_SCHEMA.online_sv_nd_agg(ind) from ' + input_table + ')';
        plpy.execute(sql);

        # Store the model learned
        plpy.execute('select MADLIB_SCHEMA.storeModel(\'' + modelname + '\')');

    return '''Finished support vector novelty detection learning on %s table. 
           ''' % (input_table)



# ---------------------------------------------------
# Function to predict the labels of points in a table
# ---------------------------------------------------
def sv_predict( input_table, col_name, modelname, output_table, parallel):
    """
    Scores the data points stored in a table using a learned support vector model.

    @param input_table Name of table/view containing the data points to be scored
    @param col_name Name of column in input_table containing the data points
    @param model_name Name under which we want to store the learned model 
    @param output_table Name of table to store the results 
    @param parallel A flag indicating whether the system should learn multiple models in parallel.
    
    """
    
    plpy.execute('drop table if exists ' + output_table);
    plpy.execute('create table ' + output_table + ' ( id int, prediction float8 )');

    if (parallel) :
        num_models_t = plpy.execute('SELECT COUNT(DISTINCT(id)) n FROM MADLIB_SCHEMA.sv_model WHERE position(\'' + modelname + '\' in id) > 0 AND \'' + modelname + '\' <> id;');
        num_models = num_models_t[0]['n'];

        sql = 'insert into ' + output_table + '(select t.id, sum(weight * MADLIB_SCHEMA.kernel(m.sv, t.' + col_name + ')) / ' + str(num_models) + ' from MADLIB_SCHEMA.sv_model m, ' + input_table + ' t where position(\'' + modelname + '\' in m.id) > 0 AND \'' + modelname + '\' <> m.id group by 1)';
        plpy.execute(sql);

    else :
        sql = 'insert into ' + output_table + '(select t.id, sum(weight * MADLIB_SCHEMA.kernel(m.sv, t.' + col_name + ')) from MADLIB_SCHEMA.sv_model m, ' + input_table + ' t where m.id = \'' + modelname + '\' group by 1)';
        plpy.execute(sql);

    return '''Finished processing data points in %s table; results are stored in %s table. 
           ''' % (input_table,output_table)

