# coding=utf-8

# This file is preprocessed with m4. Since we only want macro expansion at 
# very few places, most of the file is quoted. Macros can be used by enclosing
# them in <m4> and </m4>.
changequote(`</m4>', `<m4>') </m4>

"""
@file logRegress.py_in

@brief Logistic Regression: Driver functions

@namespace logRegress

Logistic Regression: Driver functions

@addtogroup grp_logreg

@about

Logistic regression is used to estimate probabilities of a dependent binary
variable, by fitting a stochastic model. It is one of the most commonly used
tools for applied statistics and data mining [1].

Logistic regression assumes a generalized linear model:
\f[
    E[Y] = g^{-1}(\boldsymbol c^T X)
\f]
where:
- $Y$ is the dependent variable
- \f$\boldsymbol c^T X\f$ is the linear predictor
- \f$g(x) = \ln\left( \frac{x}{1-x} \right)\f$ is the link function, with
  inverse \f$\sigma(x) := g^{-1}(x) = \frac{1}{1 + \exp(-x)} \f$

For each training data point $i$, we have a vector of
features $x_i$ and an observed class $y_i$. For ease of notation, let $Z$ be a
dependent random variable such that $Z = -1$ if $Y = 0$ and $Z = 1$ if $Y = 1$,
i.e., \f$Z := 2(Y - \frac 12)\f$. By definition,
\f$P[Z = z_i | X = x_i] = σ(z_i \cdot \boldsymbol c^T x_i)\f$.

Since logistic regression predicts probabilities, we can do maximum-likelihood
fitting: That is, we want the vector of regression coefficients
\f$\boldsymbol c\f$ to maximize
\f[
    \prod_{i=1}^n \sigma(z_i \cdot \boldsymbol c^T \boldsymbol x_i)
\f]
or, equivalently, to maximize the objective
\f[
    l(\boldsymbol c) =
        -\sum_{i=1}^n \ln(1 + \exp(-z_i \cdot \boldsymbol c^T \boldsymbol x_i))
\f]
By looking at the Hessian, we can verify that \f$l(\boldsymbol c)\f$ is convex.

There are many techniques for solving convex optimization problems. Currently,
logistic regression in MADlib can use one of two algorithms:
- Iteratively Reweighted Least Squares
- A conjugate-gradient approach, also known as Fletcher–Reeves method in the
  literature, where we use the Hestenes-Stiefel rule for calculating the step
  size.


@prereq

Implemented in C (the computation) and Python (the driver/outer loop) for
PostgreSQL/Greenplum.

@usage

-# The training data is expected to be of the following form:\n
   <tt>{TABLE|VIEW} <em>sourceName</em> ([...] <em>dependentVariable</em>
   BOOLEAN, <em>independentVariables</em> DOUBLE PRECISION[], [...])</tt>  
-# Run the logistic regression by:\n
   <tt>SELECT logregr_coef('<em>sourceName</em>', '<em>dependentVariable</em>',
   '<em>independentVariables</em>', <em>numIterations</em>,
   '<em>optimizer</em>', <em>precision</em>)</tt>\n
   The last three arguments are optional and can be omitted, in which case
   default values will be used. See compute_logregr_coef().
   
@examp

@verbatim
# select * from artificiallogreg;
 y |                           x                           
---+-------------------------------------------------------
 t | {-1.19845,1.15366,0.941779,-0.23669,-0.711024}
 f | {-0.0680205,-0.716733,-0.149781,-0.410448,-0.0843123}
 f | {-0.330021,0.222596,-0.976091,0.773816,-1.06238}
 f | {0.648293,0.286225,0.524144,-0.141286,-1.41774}
 f | {0.859484,-0.412929,-0.273368,-0.243059,0.714789}
[...]

# select madlib.logregr_coef(
#     'artificiallogreg', 'y', 'x', 20, 'irls', 0.001
# )::REAL[];
                logregr_coef                 
---------------------------------------------
 {-3.0307,3.63312,0.714105,-1.72496,1.37484}
@endverbatim

@sa namespace logRegress (documenting the driver/outer loop implemented in
	Python), function float8_cg_update_final() (documenting the
	conjugate-gradient update/iteration steps, implemented in C), function
	float8_cg_update_accum() (documenting the 
	iteratively-reweighted-least-squares update/iteration steps, implemented in
	C)

@literature

A somewhat random selection of nice write-ups, with valuable pointers into
further literature:

[1] Cosma Shalizi: Statistics 36-350: Data Mining, Lecture Notes, 18 November
    2009, http://www.stat.cmu.edu/~cshalizi/350/lectures/26/lecture-26.pdf

[2] Thomas P. Minka: A comparison of numerical optimizers for logistic
    regression, 2003 (revised Mar 26, 2007),
    http://research.microsoft.com/en-us/um/people/minka/papers/logreg/minka-logreg.pdf

[3] Paul Komarek, Andrew W. Moore: Making Logistic Regression A Core Data Mining
    Tool With TR-IRLS, IEEE International Conference on Data Mining 2005,
    pp. 685-688, http://komarix.org/ac/papers/tr-irls.short.pdf
"""

import plpy

def __runIterativeAlg(stateType, initialState, source, updateExpr,
    terminateExpr, cyclesPerIteration, maxNumIterations,
    returnExpr = "st.state"):
    """
    Driver for an iterative algorithm
    
    A general driver function for most iterative algorithms: The state between
    iterations is kept in a variable of type <tt>stateType</tt>, which is
    initialized with <tt><em>initialState</em></tt>. During each iteration, the
    SQL statement <tt>updateSQL</tt> is executed in the database. Afterwards,
    the SQL query <tt>updateSQL</tt> decides whether the algorithm terminates.
    At the end of the algorithm, the last state (or an arbitrary transformation
    as given by <tt>returnExpr</tt>) will be returned.
    
    @param stateType SQL type of the state between iterations
    @param initialState The initial value of the SQL state variable
    @param source The source relation
    @param updateExpr SQL expression that returns the new state of type
        <tt>stateType</tt>. The expression may use the replacement fields
        <tt>"{state}"</tt>, <tt>"{iteration}"</tt>, and
        <tt>"{sourceAlias}"</tt>. Source alias is an alias for the source
        relation <tt><em>source</em></tt>.
    @param terminateExpr SQL expression that returns whether the algorithm should
        terminate. The expression may use the replacement fields
        <tt>"{oldState}"</tt>, <tt>"{newState}"</tt>, and
        <tt>"{iteration}"</tt>. It must return a BOOLEAN value.
    @param cyclesPerIteration Number of aggregate function calls per iteration.
    @param maxNumIterations Maximum number of iterations. Algorithm will then
        terminate even when <tt>terminateExpr</tt> does not evaluate to \c true
    @param returnExpr An optional transformation of the final state. If omitted,
        the final state will be returned. <tt>returnExpr</tt> can be, e.g., a
        function call or access to a tuple element. Use "{state}" to refer
        to the state variable. E.g., if <tt>stateType</tt> is a tuple containing
        a field <tt>coefficients</tt>, and this should be returned as the result
        of the algorithm, pass <tt>"{state}.coefficients"</tt> here. The
        replacement field <tt>"{iteration}"</tt> can also be used.
    """

    state = "(st.state)"
    sourceAlias = "src"
    oldState = "(older.state)"
    newState = "(newer.state)"
    
    updateExpr = updateExpr.format(**locals())
    terminateExpr = terminateExpr.format(**locals())
    returnExpr = returnExpr.format(**locals())

    updateSQL = """
        INSERT INTO _madlib_iterative_alg
        SELECT
            {iteration},
            {updateExpr}
        FROM
            _madlib_iterative_alg AS st,
            {source} AS src
        WHERE
            st.iteration = {iteration} - 1
        """    
    terminateSQL = """
        SELECT
            {terminateExpr} AS should_terminate
        FROM    
        (
            SELECT state
            FROM _madlib_iterative_alg
            WHERE iteration = {iteration} - {cyclesPerIteration}
        ) AS older,
        (
            SELECT state
            FROM _madlib_iterative_alg
            WHERE iteration = {iteration}
        ) AS newer
        """
    returnSQL = """
        SELECT
            {returnExpr} AS return_value
        FROM
            _madlib_iterative_alg AS st
        WHERE
            st.iteration = {iteration}
        """

    oldMsgLevel = plpy.execute("SHOW client_min_messages")[0]['client_min_messages']
    plpy.execute("""
        SET client_min_messages = error;
        DROP TABLE IF EXISTS _madlib_iterative_alg;
		CREATE TEMPORARY TABLE _madlib_iterative_alg (
            iteration INTEGER PRIMARY KEY,
            state {stateType}
        );
        SET client_min_messages = {oldMsgLevel};
        """.format(**locals()))
    
    iteration = 0
    plpy.execute("""
        INSERT INTO _madlib_iterative_alg VALUES ({iteration}, {initialState})
        """.format(**locals()))
    while True:
        iteration = iteration + 1
        plpy.execute(updateSQL.format(**locals()))
        if iteration > cyclesPerIteration and (
            iteration >= cyclesPerIteration * maxNumIterations or
            plpy.execute(terminateSQL.format(**locals()))[0]['should_terminate']
                == True):
            break
    
    # FIXME: Returning the result set from Python code means that values
    # pass through Python (and there is a potential loss of precision by
    # conversion)
    
    returnValue = plpy.execute(returnSQL.format(**locals()))[0]['return_value']
    plpy.execute("DROP TABLE _madlib_iterative_alg")
    return returnValue


def __cg_logregr_coef(**kwargs):
    """
    Logistic regression algorithm with the conjugate-gradient method
    
    The parameters are the same as for compute_logregr_coef(), except that
    <tt>optimizer</tt> should not be set. This function sets up all SQL
    expression as needed for the conjugate-gradient method and then calls
    __runIterativeAlg().
    """
    
    stateType = "<m4>MADLIB_SCHEMA</m4>.logregr_cg_state"
    initialState = "NULL"
    source = kwargs['source']
    
    # "{state}", "{sourceAlias}", "{oldState}", and "{newState}" will not be
    # substituted here but will be passed on to __runIterativeAlg and
    # substituted there
    updateExpr = """
        <m4>MADLIB_SCHEMA</m4>.logregr_cg_step(
            {{sourceAlias}}.{depColumn},
            {{sourceAlias}}.{indepColumn},
            {{state}}
        )
        """.format(**kwargs)
    if kwargs['precision'] == 0.:
        terminateExpr = "FALSE"
    else:
        terminateExpr = """
            {{newState}}.loglikelihood - {{oldState}}.loglikelihood < {precision}
            """.format(**kwargs)
	
	# One iteration consists of two calls of the aggregate function
	# See float8_cg_update_final().
    cyclesPerIteration = 2
    maxNumIterations = kwargs['numIterations']
    returnExpr = "{state}.coef"
    return __runIterativeAlg(stateType, initialState, source, updateExpr,
        terminateExpr, cyclesPerIteration, maxNumIterations, returnExpr)


def __irls__logregr_coef(**kwargs):
    """
    Logistic regression algorithm with the iteratively-reweighted-least-squares method
    
    The parameters are the same as for compute_logregr_coef(), except that
    <tt>optimizer</tt> should not be set. This function sets up all SQL
    expression as needed for the iteratively-reweighted-least-squares method and
    then calls __runIterativeAlg().
    """
    
    stateType = "<m4>MADLIB_SCHEMA</m4>.logregr_irls_state"
    initialState = "NULL"
    source = kwargs['source']
    updateExpr = """
        <m4>MADLIB_SCHEMA</m4>.logregr_irls_step(
            {{sourceAlias}}.{depColumn},
            {{sourceAlias}}.{indepColumn},
            {{state}}
        )
        """.format(**kwargs)
    if kwargs['precision'] == 0.:
        terminateExpr = "FALSE"
    else:
        terminateExpr = """
            {{newState}}.loglikelihood - {{oldState}}.loglikelihood < {precision}
            """.format(**kwargs)

    cyclesPerIteration = 1
    maxNumIterations = kwargs['numIterations']
    returnExpr = "{state}.coef"
    return __runIterativeAlg(stateType, initialState, source, updateExpr,
        terminateExpr, cyclesPerIteration, maxNumIterations, returnExpr)
    

def compute_logregr_coef(**kwargs):
    """
    Compute logistic regression coefficients
    
    This method serves as an interface to different optimization algorithms.
    By default, iteratively reweighted least squares is used, but for data with
    a lot of columns the conjugate-gradient method might perform better.
    
    @param source Name of relation containing the training data
    @param depColumn Name of dependent column in training data (of type BOOLEAN)
    @param indepColumn Name of independent column in training data (of type
           DOUBLE PRECISION[])
    
    Optionally also provide the following:
    @param optimizer Name of the optimizer. 'newton' or 'irls': Iteratively
        reweighted least squares, 'cg': conjugate gradient (default = 'irls')
    @param numIterations Maximum number of iterations (default = 20)
    @param precision Terminate if two consecutive iterations have a difference 
           in the log-likelihood of less than <tt>precision</tt>. In other
           words, we terminate if the objective function value has converged.
           If this parameter is 0.0, then the algorithm will not check for
           convergence and only terminate after <tt><em>numIterations</em></tt>
           iterations.
    
    @return array with coefficients in case of convergence, otherwise None
    
    """
    if not 'optimizer' in kwargs:
        kwargs.update(optimizer = 'irls')
    if not 'numIterations' in kwargs:
        kwargs.update(numIterations = 20)
    if not 'precision' in kwargs:
        kwargs.update(precision = 0.0001)
        
    if kwargs['optimizer'] == 'cg':
        return __cg_logregr_coef(**kwargs)
    elif kwargs['optimizer'] in ['irls', 'newton']:
        return __irls__logregr_coef(**kwargs)
    else:
        plpy.error("Unknown optimizer requested. Must be 'newton'/'irls' or 'cg'")
    
    return None

# The m4 preprocessor complains if eof is reach in quoted mode.
<m4>