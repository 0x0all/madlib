# coding=utf-8

# This file is preprocessed with m4. Since we only want macro expansion at 
# very few places, most of the file is quoted. Macros can be used by enclosing
# them in <m4> and </m4>.
changequote(`</m4>', `<m4>') </m4>

"""
@file logRegress.py

@brief Logistic Regression: Driver functions

@namespace logRegress

Logistic Regression: Driver functions

@defgroup logRegress Logistic Regression
@ingroup regression

@about

Logistic regression is used to estimate probabilities of a dependent binary
variable, by fitting a stochastic model. It is one of the most commonly used
tools for applied statistics and data mining [1].

Logistic regression assumes a generalized linear model:
\f[
    E[Y] = g^{-1}(\boldsymbol c^T X)
\f]
where:
- $Y$ is the dependent variable
- \f$\boldsymbol c^T X\f$ is the linear predictor
- \f$g(x) = \ln\left( \frac{x}{1-x} \right)\f$ is the link function, with
  inverse \f$\sigma(x) := g^{-1}(x) = \frac{1}{1 + \exp(-x)} \f$

For each training data point $i$, we have a vector of
features $x_i$ and an observed class $y_i$. For ease of notation, let $Z$ be a
dependent random variable such that $Z = -1$ if $Y = 0$ and $Z = 1$ if $Y = 1$,
i.e., \f$Z := 2(Y - \frac 12)\f$. By definition,
\f$P[Z = z_i | X = x_i] = σ(z_i \cdot \boldsymbol c^T x_i)\f$.

Since logistic regression predicts probabilities, we can do maximum-likelihood
fitting: That is, we want the vector of regression coefficients
\f$\boldsymbol c\f$ to maximize
\f[
    \prod_{i=1}^n \sigma(z_i \cdot \boldsymbol c^T \boldsymbol x_i)
\f]
or, equivalently, to maximize the objective
\f[
    l(\boldsymbol c) =
        -\sum_{i=1}^n \ln(1 + \exp(-z_i \cdot \boldsymbol c^T \boldsymbol x_i))
\f]
By looking at the Hessian, we can verify that \f$l(\boldsymbol c)\f$ is convex.

There are many techniques for solving convex optimization problems. Currently,
logistic regression in MADlib can use one of two algorithms:
- Iteratively Reweighted Least Squares
- A conjugate-gradient approach, also known as Fletcher–Reeves method in the
  literature, where we use the Hestenes-Stiefel rule for calculating the step
  size.


@prereq

Implemented in C for PostgreSQL/Greenplum.

@usage

-# The training data is expected to be of the following form:\n
   <tt>{TABLE|VIEW} <em>sourceName</em> ([...] <em>dependentVariable</em>
   BOOLEAN, <em>independentVariables</em> DOUBLE PRECISION[], [...])</tt>  
-# Run the logistic regression by:\n
   <tt>SELECT logregr_coef('<em>sourceName</em>', '<em>dependentVariable</em>',
   '<em>independentVariables</em>')</tt>
   
@literature

[1] Cosma Shalizi, Statistics 36-350: Data Mining, Lecture Notes, 18 November
    2009, http://www.stat.cmu.edu/~cshalizi/350/lectures/26/lecture-26.pdf

[2] Thomas P. Minka, A comparison of numerical optimizers for logistic
    regression, 2003 (revised Mar 26, 2007),
    http://research.microsoft.com/en-us/um/people/minka/papers/logreg/minka-logreg.pdf
"""

import plpy

def __runIterativeAlg(stateType, initialState, source, updateExpr,
    terminateExpr, numIterations, returnExpr = "st.state"):
    """
    Driver for an iterative algorithm
    
    A general driver function for most iterative algorithms: The state between
    iterations is kept in a variable of type <tt>stateType</tt>, which is
    initialized with <tt><em>initialState</em></tt>. During each iteration, the
    SQL statement <tt>updateSQL</tt> is executed in the database. Afterwards,
    the SQL query <tt>updateSQL</tt> decides whether the algorithm terminates.
    At the end of the algorithm, the last state (or an arbitrary transformation
    as given by <tt>returnExpr</tt>) will be returned.
    
    @param stateType SQL type of the state between iterations
    @param initialState The initial value of the SQL state variable
    @param source The source relation
    @param updateExpr SQL expression that returns the new state of type
        <tt>stateType</tt>. The expression may use the replacement fields
        <tt>"{state}"</tt>, <tt>"{iteration}"</tt>, and
        <tt>"{sourceAlias}"</tt>. Source alias is an alias for the source
        relation <tt><em>source</em></tt>.
    @param terminateExpr SQL expression that returns whether the algorithm
        should terminate. The expression may use the replacement fields
        <tt>"{oldState}"</tt>, <tt>"{newState}"</tt>, and
        <tt>"{iteration}"</tt>. It must return a BOOLEAN value.
    @param numIterations Maximum number of iterations. Algorithm will then
        terminate even when <tt>terminateExpr</tt> does not evaluate to \c true
    @param returnExpr An optional transformation of the final state. If omitted,
        the final state will be returned. <tt>returnExpr</tt> can be, e.g., a
        function call or access to a tuple element. Use "{state}" to refer
        to the state variable. E.g., if <tt>stateType</tt> is a tuple containing
        a field <tt>coefficients</tt>, and this should be returned as the result
        of the algorithm, pass <tt>"{state}.coefficients"</tt> here. The
        replacement field <tt>"{iteration}"</tt> can also be used.
    """

    state = "(st.state)"
    sourceAlias = "src"
    oldState = "(old.state)"
    newState = "(new.state)"
    
    updateExpr = updateExpr.format(**locals())
    terminateExpr = terminateExpr.format(**locals())
    returnExpr = returnExpr.format(**locals())

    updateSQL = """
        INSERT INTO _madlib_iterative_alg
        SELECT
            {iteration},
            {updateExpr}
        FROM
            _madlib_iterative_alg AS st,
            {source} AS src
        WHERE
            st.iteration = {iteration} - 1
        """    
    terminateSQL = """
        SELECT
            {terminateExpr}
        FROM    
        (
            SELECT state
            FROM _madlib_iterative_alg
            WHERE iteration = {iteration} - 1
        ) AS old,
        (
            SELECT state
            FROM _madlib_iterative_alg
            WHERE iteration = {iteration}
        ) AS new
        """
    returnSQL = """
        SELECT
            {returnExpr} AS return_value
        FROM
            _madlib_iterative_alg AS st
        WHERE
            st.iteration = {iteration}
        """

    oldMsgLevel = plpy.execute("SHOW client_min_messages")[0]['client_min_messages']
    plpy.execute("""
        SET client_min_messages = error;
        DROP TABLE IF EXISTS _madlib_iterative_alg;
		CREATE TABLE _madlib_iterative_alg (
        --CREATE TEMPORARY TABLE _madlib_iterative_alg (
            iteration INTEGER PRIMARY KEY,
            state {stateType}
        );
        SET client_min_messages = {oldMsgLevel};
        """.format(**locals()))
    
    iteration = 0
    plpy.execute("""
        INSERT INTO _madlib_iterative_alg VALUES ({iteration}, {initialState})
        """.format(**locals()))
    while True:
        iteration = iteration + 1
        plpy.execute(updateSQL.format(**locals()))
        if (iteration >= numIterations or
            plpy.execute(terminateSQL.format(**locals())) == True):
            break
    
    # FIXME: Returning the result set from Python code means that values
    # pass through Python (and there is a potential loss of precision by
    # conversion)
    
    returnValue = plpy.execute(returnSQL.format(**locals()))[0]['return_value']
#    plpy.execute("DROP TABLE _madlib_iterative_alg")
    return returnValue


def __cg_logreg_coef(**kwargs):
    """
    Logistic regression algorithm with the conjugate-gradient method
    
    The parameters are the same as for compute_logreg_coef(), except that
    <tt>optimizer</tt> should not be set. This function sets up all SQL
    expression as needed for the conjugate-gradient method and then calls
    __runIterativeAlg().
    """
    
    stateType = "<m4>MADLIB_SCHEMA</m4>.LRegrState"
    initialState = "NULL"
    source = kwargs['source']
    
    # "{state}", "{sourceAlias}", "{oldState}", and "{newState}" will not be
    # substituted here but will be passed on to __runIterativeAlg and
    # substituted there
    updateExpr = """
        <m4>MADLIB_SCHEMA</m4>.logreg_cg_step(
            {{sourceAlias}}.{depColumn},
            {{sourceAlias}}.{indepColumn},
            {{state}}
        )
        """.format(**kwargs)
    terminateExpr = """
        <m4>MADLIB_SCHEMA</m4>.logreg_should_terminate(
            {{oldState}}.coef, {{newState}}.coef,
            '{optimizer}', {precision})
        """.format(**kwargs)
	
	# One iteration consists of two calls of the aggregate function
	# See float8_cg_update_final().
    numIterations = 2 * kwargs['numIterations']
    returnExpr = "{state}.coef"
    return __runIterativeAlg(stateType, initialState, source, updateExpr,
        terminateExpr, numIterations, returnExpr)


def __irls__logreg_coef(**kwargs):
    """
    Logistic regression algorithm with the iteratively-reweighted-least-squares method
    
    The parameters are the same as for compute_logreg_coef(), except that
    <tt>optimizer</tt> should not be set. This function sets up all SQL
    expression as needed for the iteratively-reweighted-least-squares method and
    then calls __runIterativeAlg().
    """
    
    stateType = "DOUBLE PRECISION[]"
    initialState = "NULL"
    source = kwargs['source']
    updateExpr = """
        <m4>MADLIB_SCHEMA</m4>.logreg_irls_step(
            {{sourceAlias}}.{depColumn},
            {{sourceAlias}}.{indepColumn},
            {{state}}
        )
        """.format(**kwargs)
    terminateExpr = """
        <m4>MADLIB_SCHEMA</m4>.logreg_should_terminate(
            {{oldState}}, {{newState}},
            '{optimizer}', {precision})
        """.format(**kwargs)
    numIterations = kwargs['numIterations']
    returnExpr = "{state}"
    return __runIterativeAlg(stateType, initialState, source, updateExpr,
        terminateExpr, numIterations, returnExpr)
    

def compute_logreg_coef(**kwargs):
    """
    Compute logistic regression coefficients
    
    This method serves as an interface to different optimization algorithms.
    
    @todo
    - It would be great if this method automatically chose the algorithm
      that is expected to work best.
    
    @param source Name of relation containing the training data
    @param depColumn Name of dependent column in training data (of type BOOLEAN)
    @param indepColumn Name of independent column in training data (of type
           DOUBLE PRECISION[])
    
    Optionally also provide the following:
    @param optimizer Name of the optimizer. 'newton' or 'irls': Iteratively reweighted least squares,
           'cg': conjugate gradient (default = 'cg')
    @param numIterations Maximum number of iterations (default = 20)
    @param precision Terminate if two consecutive iterations have a difference 
           of coefficients with \f$\ell^2\f$-norm less than <tt>precision</tt>.
    
    @return array with coefficients in case of convergence, otherwise None
    
    """
    if not 'optimizer' in kwargs:
        kwargs.update(optimizer = 'cg')
    if not 'numIterations' in kwargs:
        kwargs.update(numIterations = 20)
    if not 'precision' in kwargs:
        kwargs.update(precision = 0.0001)
        
    if kwargs['optimizer'] == 'cg':
        return __cg_logreg_coef(**kwargs)
    elif kwargs['optimizer'] in ['irls', 'newton']:
        return __irls__logreg_coef(**kwargs)
    else:
        plpy.error("Unknown optimizer requested. Must be 'newton'/'irls' or 'cg'")
    
    return None

# The m4 preprocessor complains if eof is reach in quoted mode.
<m4>